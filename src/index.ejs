<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>

  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- <script src="dependencies/twgl.min.js"></script> -->
</head>


<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>A visual introduction to Gaussian Belief Propagation</h1>
    <p>A Framework for Distributed Inference with Emerging Hardware. </p>
  </d-title>



  <d-article>

    <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <ul>
          <li><a href="#the-need-for-probabilistic-inference">The need for probabilistic inference </a></li>
          <li><a href="#gaussian-belief-propagation">Gaussian Belief Propagation</a></li>
        </ul>
        <div><a href="#technical-introduction">Technical Introduction</a></div>
        <ul>
          <li><a href="#factor-graphs">Factor Graphs</a></li>
          <li><a href="#multivariate-gaussians">Multivariate Gaussians</a></li>
          <li><a href="#from-probabilistic-inference-to-linear-algebra">From Probabilistic Inference to Linear Algebra</a></li>
          <li><a href="#the-gaussian-belief-propagation-algorithm">The Gaussian Belief Propagation Algorithm</a></li>
        </ul>
        <div><a href="#beyond-the-standard-algorithm">Beyond the standard algorithm</a></div>
        <ul>
          <li><a href="#non-linear-factors">Non-linear factors</a></li>
          <li><a href="#local-updates-and-scheduling">Local updates and Scheduling</a></li>
          <li><a href="#multiscale-learning">Multiscale Learning</a></li>
        </ul>
        <div><a href="#related-work">Related Work</a></div>
        <div><a href="#discussion">Discussion</a></div>
      </nav>
    </d-contents>

    <g>

    <h2 id="introduction">Introduction </h2>

    <p>
      The "Bitter Lesson" of machine learning (ML) research has been that "general methods that leverage computation are ultimately the most effective, and by a large margin" <d-cite key="Sutton:BitterLesson2019"></d-cite>. At the time of writing, the state-of-the-art across most ML benchmarks is dominated by GPU driven deep learning.
    </p>
    <p>
      The effectiveness of the DL/GPU approach stems largely from the close structural coupling of hardware, algorithm and the underlying task. 
      Convolutional Neural Networks (CNN; <d-cite key="LeCun:etal:IEEE1998"></d-cite>) operate by learning translation invariant spatial filters, which are multiplied over small patches of adjacent pixels, exploiting the fact that correlations in natural images are strongly local. 
      This local structure (also present in general matrix-multiply driven DL) aligns strongly with the structure of GPUs and has meant that the scale and performance of DL has grown with developments in multicore compute.    
    </p>
    <p>
      As ML is increasingly embedded into the physical world, power consumption and communication costs both within and across systems will become limiting factors. 
      Both can be minimized by co-developing algorithms and hardware that minimize "bits x millimetres" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>, by storing data nearer to the location at which it's operated on <d-cite key="Sze:Survey2017"></d-cite>.
      Towards this goal, Sutter identifies the emergence of a "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of parallel, heterogeneous, and distributed compute.
    </p>
    <p>
      To operate in the real world, ML systems of the future will also need to be highly inhomogeneous in their representation. 
      General purpose perception systems will represent the world not just at the level of sequences of regular image arrays, but as graphs of semantic objects with continuously changing relationships, growing or contracting in size as new data is added or task demands fluctuate.
    </p>
    <p>
      Stated simply, so far, the inductive biases present in DL, which themselves reflect the task structure, have been well mirrored in GPU hardware. 
      However we see a gradual shift in both hardware and the structure of tasks in AI and believe it is time to think carefully about which algorithms can occupy a similar sweet spot against the backdrop of increasingly heterogeneous compute and representations. 
    </p>
    
    </g>
    
    <h3 id="the-need-for-probabilistic-inference">The need for probabilistic inference </h3>

    <p>
      We believe that the a core component of future systems will be the ability to represent the world probabilistically. 
      Systems that can take into account the relative uncertainty of new data are able to more appropriately update their existing beliefs, learn from less data (meaning less computation and lower energy costs; Ref.) and mitigate catastrophic forgetting in continually learning settings (ref.). 
    </p>
    <p>
      Probabilistic representations will also be important where interpretability is desired <d-cite key="Ghahramani:Nature2015"></d-cite> and essential in "Spatial AI" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> applications such as autonomous transport and robotics, which need to act safely in the physical world.
    </p>

    <h3 id="gaussian-belief-propagation">Gaussian Belief Propagation </h3>

    <p>
      To support this "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of highly specialized but interconnected systems, we argue for probabilistic graphical models (PGM) as the root structure. 
      PGMs have a rich research literature in ML, but have not been successfully scaled in their general undirected (loopy) form (FIG: directed vs. undirected graphs). 
      However, we believe this will change as hardware <d-cite key="Graphcore"></d-cite> and tasks evolve and deserves more attention by researchers to further accelerate this shift <d-cite key="Hooker:hardware2020"></d-cite> (Fig: Co-ev.).
    </p>
    <p>
      In this article, we discuss Gaussian Belief Propagation (GBP) as a strong general purpose algorithmic and representational framework for large scale distributed inference. 
      GBP is a special case of loopy belief propagation (BP), which operates on a per node basis by purely local message passing. 
      Inference by GBP, unlike backpropagation, does not require access to a global objective function, making it easily distributable (ref. biological backprop; Fig. ??). 
      GBP is flexible to changes in graph structure, in that nodes need only to take into account the messages they receive. GBP can also operate asynchronously, without the need for a global clock. 
      This is especially desirable in large or distributed systems where communication delays become non-negligible relative to clock-speed. 
    </p>
    <p>
      This iterative, asynchronous operation will enable large graphs that are 'always approximately correct', in the sense that local clusters of nodes may be well converged relative to one another, despite misalignment with more distant clusters. This could, for example enable an embodied agent or robot to perform local obstacle avoidance without needing the global graph to have converged. This property is also attractive from an energy consumption perspective; computation can be focused on regions relevant to the current task demand to enable 'just in time' convergence <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>
    <p>
      Unlike general loopy BP, GBP is convergent under well defined conditions <d-cite key="Bickson:PhDThesis:2008"></d-cite>.
      In this article, we will show that GBP is intimately linked to the essential problem of solving linear systems of equations $Ax = b$ (Section X). 
      We will also explore how introducing non-linear factors can extend the framework for more flexible representation (Section X), how message scheduling affects convergence (Section X) and how communication overheads can be minimized by adding hierarchical structure (Section X). 
    </p>


    <h2 id="technical-introduction">Technical Introduction </h2>

    <p>
      Belief propagation was developed to perform exact inference in probabilistic models with tree-like graphical structure (Pearl, 1988). 
      Although it will work for models with arbitrary conditional independence structure, including cycles or "loops" <d-cite key="Murphy:etal:1999"></d-cite>, it is most effective when applied to those with non-uniform but sparse dependencies. 
    </p>

    <h3 id="factor-graphs">Factor Graphs </h3>

    <p>
      The Hammersley-Clifford theorem tells us that any positive joint distribution can be represented as a product of factors, one per clique, where a clique is a subset of variables in which each variable is connected to all others. 
      A node, edge and triangle are examples of 1-,2- and 3-cliques, respectively. 
    </p>
    <p>
      <d-figure class="right-d-figure" id="factor_graph"></d-figure>

      This theorem allows us to visualize any distribution as a factor graph. 
      Factor graphs are a useful representation for inspecting problems and simplify belief propagation to message passing on a the graph.
      An example of simple factor graph is shown in the figure on the right. 
      We use circles and squares to represent variable and factor nodes, respectively. 
      Factor graphs show both the conditional independence structure 
      (the lack of an edge means conditional independence 
      <d-footnote>Mathematically, conditional independence means $\Lambda_{i,j} = 0 \;\; \Longleftrightarrow \;\; P(x_i, x_j | X_{-ij}) = P(x_i | X_{-ij}) P(x_j | X_{-ij}) \;\; \Longleftrightarrow \;\; x_i \bot x_j | X_{-ij}$</d-footnote>
      ) and the factorisation of the distribution. 
    </p>
    <p>
      In probabilistic inference, we often represent the posterior density $P(x|z) = \frac{P(z|x) P(x)}{P(z)}$ as a factor graph. Here $x$ is the state we want to estimate and $z$ are observations that depend in some way on the state. 
      The factors in the graph are either likelihoods of an observation given the variables (terms from $P(z|x)$) or priors over the variables (terms from $P(x)$). 
    </p>

    <h3 id="multivariate-gaussians">Multivariate Gaussians</h2>

    <p>
      The Gaussian distribution is ubiquitously used to model events in the world with great practical success because of its statistical properties <d-cite key="Jaynes:probability2003"></d-cite>:
      (1) Gaussians have maximal entropy for a given mean and variance and (2) the sum of many independent random variables tends towards being Gaussian distributed by the Central Limit Theorem.
      <d-footnote>If we only know the first two moments of a distribution, the principle of maximum entropy or applying Occam's Razor tells us to use a Gaussian model while if we only observe magnitudes, the central limit theorem tells us that the Gaussian functional form is still often accurate.</d-footnote>
      .
      In this article, we discuss Gaussian Belief Propagation in which all factors and therefore the joint distribution are modelled as Gaussians.
    </p>
    <p>
      We should briefly mention here that it is well known that Gaussians underestimate the true distribution in the tails for many real events and we will discuss ways to handle this later within GBP in section (nonlinear factors). Also there are formulations of BP that directly handle non-Gaussian distributions <d-cite key="Sudderth:2010nonparametric"></d-cite>, however we focus on GBP for simplicity.
    </p>
    <p>
      There are two common parameterisation of multivariate Gaussian distributions: the <i>moments form</i> <d-footnote>It is called the moments form as it is parameterised by the first moment and second central moments of the distribution.</d-footnote> and the <i>canonical form</i>. 
    </p>

    <d-figure id="gaussian_equations" ></d-figure>

    <p>
      When representing posterior distributions which are multivariate Gaussians we usually use the canonical form due to the relation between the precision matrix and graph that we will explore with the help of the interactive figure below. 
      You can click on the question mark in the top right of the figure for some hints and explanations.
    </p>
    <p>
      The precision matrix of the canonical form describes direct associations or conditional dependence between variables, meaning if entry $i,j$ is zero then there is no factor that directly connects $x_i$ and $x_j$ in the graph. 
      You can see this in the figure where $\Lambda_{13}=\Lambda_{13}=0$ and $x_1$ and $x_3$ have no factor directly connecting them.
      On the other hand, the covariance matrix describes induced correlations between variables and is dense as long as the graph is only single connected component. 
      The covariance form is also unable to represent unconstrained distributions, as you can see by selecting the unanchored configuration in the figure. 
    </p>

    <d-figure id="gaussian_gm" class="subgrid"></d-figure>

    <p>
      Different transformations are more easily carried out in different forms. 
      Taking the product of two Gaussians and conditioning are cheap in the canonical form and expensive in the moments form whereas the opposite is true for marginalisation
      <d-footnote>
        The Gaussian distribution family is closed under marginalisation and conditioning while the product of two Gaussians in general yields an un-normalised Gaussian.
      </d-footnote>.
    </p>

    <h3 id="from-probabilistic-inference-to-linear-algebra">From Probabilistic Inference to Linear Algebra</h2>

    <p>
      In probabilistic inference we would often like to find the variable configuration that maximises the posterior along with the confidence around each variable estimate. The process of finding just the point estimate is called <i>maximum a posteriori</i> (MAP) estimation and for Gaussian posteriors, it can be shown to be equivalent to least squares optimisation, solving the linear system $Ax = b$ and converting from the canonical form to the moments form:
    </p>

    <d-figure id="probinf_eqns"></d-figure>

    <p>
      Directly solving the linear system for $x_{\text{\tiny MAP}}$ by inverting $\Lambda$ gives the confidence which is the marginal variances or the diagonal elements of the joint covariance matrix $\Sigma = \Lambda^{-1}$. 
      For large systems, this inversion can be very expensive and Gaussian Belief Propagation is an iterative methods that estimates the full marginal distributions, giving both the MAP estimate (the marginal means) and the confidence (the marginal variances) with potentially lower computational cost. We discuss related linear solvers and non-linear least squares optimisation methods in Section (rel work).      
    </p>

    <h3 id="the-gaussian-belief-propagation-algorithm">The Gaussian Belief Propagation Algorithm</h2>

    <p>
      To recap, Belief Propagation is a probabilistic inference algorithm that computes the per-node marginal distributions from the joint distribution.
      It does so by iterative message passing on the factor graph, alternating between factor-to-variable and variable-to-factor message passing.
      The algorithm is fully described by 2 types of message passing functions and one update function which aggregates incoming messages at a variable node to estimate the marginal distribution or belief. 
    </p>

    <d-figure id="gbp_equations" class="subgrid"></d-figure>

    <p>
      When all factors are Gaussian, the joint distribution and all beliefs are also Gaussian.
      The belief estimates the marginals in the canonical form which can then cheaply be converted into the moments form to give the desired marginal means and variances. 
      As GBP proceeds, messages are passed through the graph and the beliefs converge towards the marginal distributions. The following interactive figure presents an overview of GBP for a simple grid alignment problem. 
    </p>

    <d-figure id="message_passing"></d-figure>

    <h2 id="beyond-the-standard-algorithm">Beyond the standard algorithm </h2>

    <h3 id="non-linear-factors">Non-linear factors</h3>

    <d-figure id="factor_linearisation"></d-figure>


    <h3 id="local-updates-and-scheduling">Local updates and Scheduling</h3>
    
    <d-figure id="attentiongl"></d-figure>

    <h3 id="multiscale-learning">Multiscale Learning</h3>

    <figure class="l-page-outset">
      <d-figure id="surface_fitting"></d-figure>
    </figure>

    <figure class="l-page-outset">
      <d-figure id="playground"></d-figure>
    </figure>
  
    <h2 id="related-work">Related work</h2>

    <h2 id="discussion">Discussion</h2>
  

  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> 
    </p>

    <p>
      <b>Writing & Diagrams:</b> 
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
