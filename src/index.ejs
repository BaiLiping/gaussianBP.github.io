<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>

  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- <script src="dependencies/twgl.min.js"></script> -->
</head>


<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>A visual introduction to Gaussian Belief Propagation</h1>
    <p>A Framework for Distributed Inference with Emerging Hardware. </p>
  </d-title>



  <d-article>

    <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <!-- <ul>
          <li><a href="#the-need-for-probabilistic-inference">The need for probabilistic inference </a></li>
          <li><a href="#gaussian-belief-propagation">Gaussian Belief Propagation</a></li>
        </ul> -->
        <div><a href="#technical-introduction">Technical Introduction</a></div>
        <!-- <ul>
          <li><a href="#probabilistic-inference">Probabilistic Inference</a></li>
          <li><a href="#factor-graphs">Factor Graphs</a></li>
          <li><a href="#the-belief-propagation-algorithm">The Belief Propagation Algorithm</a></li>
          <li><a href="#gaussian-models">Gaussian Models</a></li>
          <li><a href="#from-gaussian-inference-to-linear-algebra">From Gaussian Inference to Linear Algebra</a></li>
          <li><a href="#gaussian-belief-propagation-ti">Gaussian Belief Propagation</a></li>
        </ul> -->
        <div><a href="#beyond-the-standard-algorithm">Beyond the standard algorithm</a></div>
        <ul>
          <li><a href="#non-linear-factors">Non-linear factors</a></li>
          <li><a href="#robust-loss-functions">Robust Loss Functions</li>
          <li><a href="#local-updates-and-scheduling">Local updates and Scheduling</a></li>
          <li><a href="#multiscale-learning">Multiscale Learning</a></li>
        </ul>
        <div><a href="#related-methods">Related Methods</a></div>
        <div><a href="#discussion">Discussion</a></div>
        <div><a href="#supp-playground">Supplementary Playground</a></div>
      </nav>
    </d-contents>

    <g>

    <p>
      In this article, we present a visual introduction to Gaussian Belief Propagation (GBP), a probabilistic and iterative approximate inference algorithm.
      GBP operates by local message passing, naturally exploiting the structure of the task, and can scale more favourably than algorithms that require access to global information. 
      Although GBP was first studied in the 80s, we believe that as hardware is becoming more parallel and distributed and our world models are becoming more inhomogeneous, now is the right time to revisit GBP which we argue has the right computational properties to act as the glue in future machine learning systems.
    </p>

    <h2 id="introduction">Introduction </h2>

    <p>
      The "Bitter Lesson" of machine learning (ML) research has been that "general methods that leverage computation are ultimately the most effective, and by a large margin" <d-cite key="Sutton:BitterLesson2019"></d-cite>. At the time of writing, the state-of-the-art across most ML benchmarks is dominated by GPU driven deep learning (DL).
    </p>

    <p>
      The effectiveness of the DL/GPU approach draws significantly from a close structural coupling of hardware, algorithm and underlying task. Convolutional Neural Networks (CNN) <d-cite key="LeCun:etal:IEEE1998"></d-cite> learn translation invariant functions by applying spatial filters over patches of adjacent pixels, exploiting both the fact that correlations in natural images are strongly local and that similar features appear in multiple locations. Importantly, this structure is also leveraged in the computation - the filter dot-products can be parallelized and carried out efficiently by GPUs <d-footnote>The sequential filter dot-products can be implemented as a single matrix multiplication (e.g. by Fourier transform or reshaping <d-cite key="chetlur2014cudnn"></d-cite>) which is carried out efficiently in parallel on a GPU. Data transfer is minimized since the filter array is transformed within the GPU.</d-footnote>. 
    </p>

    <h3 id="mirroring">Mirroring inductive biases in hardware</h3>

    <p class="note">
      This still needs work - is hard to understand intro. Needs more signposting like...
      As we seek to replicate the success of the DL/GPU approach, we take stock of the current trends in hardware and representation.
      Don't make out like we are proposing something better than DL / GPU, we are simply taking ideas from it.
    </p>

    <p>
      Data transfer, not multipy-and-accumlate (MAC) operations are most often the bottleneck to processing speed, and dominate the power consumption of a typical deep learning model <d-cite key="Sze:Survey2017"></d-cite>. As ML is increasingly embedded into the physical world, communication costs (and delays) within and across devices will become significant limiting factors if not implemented carefully. A promising solution is to co-develop algorithms and hardware that minimize "bits x millimetres" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>, by storing data nearer to the location at which it's operated on <d-cite key="Sze:Survey2017"></d-cite>.
    </p>
    <p>
      To operate effectively in the real world, ML systems of the future will also need to be highly heterogeneous in their representation. General purpose perception systems will represent the world not just at the level of sequences of regular image arrays, but as graphs of semantic objects with continuously changing relationships, growing or contracting in size as new data is added or task demands fluctuate.
    </p>
    <p>
      These requirements point to a future "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of parallel, heterogeneous, distributed and asynchronous computing systems. As has been the case for algorithmic inductive biases, hand-designing these hardware will likely pose a significant challenge. Instead, we believe that what is needed is a flexible algorithmic/computational structure that can adapt to the demands of the task.
    </p>
    
    </g>
    
    <h3 id="the-need-for-probabilistic-inference">The need for probabilistic inference </h3>

    <p>
      Moreover, we believe that the the "glue" that will allow heterogeneous future systems to interact self-consistely will be the ability to represent the world probabilistically. Systems that can take into account the relative uncertainty of new data are able to more appropriately update their existing beliefs, learn from less data (meaning less computation and lower energy costs) and mitigate catastrophic forgetting in continual learning settings <span class="note">(needs refs)</span>. 
    </p>

    <p>
      Probabilistic representations will also be important where interpretability is desired <d-cite key="Ghahramani:Nature2015"></d-cite> and essential in "Spatial AI" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> applications such as autonomous transport and robotics, which need to act safely in the physical world.
    </p>

    <p>
      To support this "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of heterogeneous but interconnected systems, we argue for probabilistic graphical models (PGM) as the root structure. Inference in PGMs has a rich research literature in ML but has not been successfully scaled for arbitrary (loopy) graphs. 
    </p>

    <h3 id="gaussian-belief-propagation">Gaussian Belief Propagation </h3>

    <p class="note">
      Andy - takes a long time to get to the first mention of GBP.
      Andy - stronger - GBP is special; GBP with robust factors and linearisation is much more general than most people would think.
    </p>

    <p>
      Belief propagation (BP) <d-cite key="Pearl:book1988, kschischang2001factor"></d-cite> is a probabilistic inference algorithm that operates by local message passing on factor graphs.
      BP has been applied with great success to error-correcting codes <d-cite key="mceliece1998turbo"></d-cite> however perhaps due to its lack of convergence guarantees has not been applied more broadly in other domains.
      Gaussian belief propagation is a special case of BP with more extensive mathematical guarantees and strong empirical performance <span class="note">(refs for both)</span>.
      BP has seen a recent resurgence <d-cite key="george2017generative, satorras2021neural, kuck2020belief, Ortiz:etal:CVPR2020, opipari2021differentiable"></d-cite> since its initial development by Pearl in the 80s <d-cite key="Pearl:book1988"></d-cite>.
      Unlike Graph neural networks <d-cite key="scarselli2008graph, bronstein2017geometric, battaglia2018relational"></d-cite> which learn edge and node updates that are applied over a fixed number of message passing steps, BP applies probabilistic message passing updates with iterative convergent behaviour. 
    </p>

    <p>
      In this article, we discuss Gaussian Belief Propagation as a strong general purpose algorithmic and representational framework for large scale distributed inference. 
      GBP has desirable properties with respect to the vision of distributed future systems that we presented above.
      The key properties of GBP are that it is:
    </p>

    <p>
      <ol>
        <li>
          <b>Local</b> - updates are based on local message passing in factor graphs.
        </li>
        <ul>
          <li>
            GBP can be trivially distributed and leverage any extra avaliable compute whether multicore or distributed. On new multicore graph processors <d-cite key="Lacey:IPUBenchmarks2019, gui2019survey"></d-cite> GBP has been shown to achieve rapid performance <d-cite key="Ortiz:etal:CVPR2020"></d-cite>, while GBP is well-suited to edge / federated computing in distributed systems.
          </li>
          <li>
            GBP naturally exploits any structure in the problem as communication is only between conditionally dependent nodes. 
            <!-- For comparison, many other methods assume a fixed structure for efficiency -->
          </li>
          <li>
            GBP can operate in an "attention-driven" fashion in which computation is focused on regions relevant to the current task to enable "just in time" convergence <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>. This is also attractive from an energy consumption perspective.
          </li>

        </ul>

        <li>
          <b>Probabilistic</b> - it estimates uncertainties.
        </li>
        
        <li>
          <b>Iterative </b> - the underlying problem can be arbitrarily edited while inference is continually underway in the background. 
        </li>
        <li>
          <b>Asynchronous</b> - convergence can be reached via asynchronous updates. This is important in distributed systems without a global clock or where communication delays are non-negligible <span class="note">(Edge computing ref)</span>.
        </li>

      </ol>
    </p>

    <p>
      In the remainder of the article, we first introduce the GBP algorithm and show that it is intimately linked to solving a linear system of equations $Ax = b$. 
      We then explore 4 key practical details for applying GBP to real problems: extending GBP to handle <a href="#non-linear-factors">non-linear factors</a> and <a href="#robust-loss-functions">robust loss functions</a>, using <a href="#local-updates-and-scheduling">local message schedules</a> and lastly how <a href="#multiscale-learning">hierarchical structure</a> can help convergence. 
    </p>

    <h2 id="technical-introduction">Technical Introduction </h2>

    <h3 id="probabilistic-inference">Probabilistic Inference </h3>

    <p>
      Inference is the problem of estimating statistical properties of an unknown quantity $X$ from known quantities $D$.
      For example, inferring the weather tomorrow (X) from historic data (D) or inferring the 3D structure of an environment (X) from a video sequence (D).
    </p>
    <p>
      The Bayesian method for inference, proceeds by first designing a probabilistic model $p(X, D)$ that describes how all the variables are produced and then using the sum and product rules of probability
      <d-footnote>
      The sum rule is $p(X) = \sum_Y p(X, Y)$ and the product rule is $p(X, Y) = p(Y \rvert X) p(X)$. 
      </d-footnote>
      to form the posterior distribution $p(X \rvert D) = \frac{p(X, D)}{p(D)}$.
      The posterior distribution summarises our belief about $X$ after seeing $D$ and can be used for Bayesian decision making or other downstream tasks. 
    </p>
    <p>
      From the posterior, we can compute things like (i) the most likely configuration of all the variables $X_{\text{MAP}} = \text{arg max}_X p(X \rvert D)$ or (ii) the marginal posteriors $p(x_i \rvert D) = \sum_{X \setminus x_i} p(X \rvert D)$ which summarise our belief about each individual variable after seeing $D$. 
      These two calculations are known as <b>MAP inference</b> and <b>marginal inference</b> respectively.
      An important difference to note is that MAP inference produces a point estimate while marginal inference retains information about uncertainty.
    </p>

    <h3 id="factor-graphs">Factor Graphs </h3>

    <p>
      The <a href="https://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem" target="_blank">Hammersley-Clifford theorem</a> tells us that any positive joint distribution $p(X)$ can be represented as a product of factors $f_i$, one per clique, where a clique is a subset of variables $X_i$ in which each variable is connected to all others: 
    </p>
    <d-math block="">
      p(X) = \prod_i f_i(X_i)
      ~.
    </d-math>
    <g>
      <d-figure class="right-d-figure" id="factor_graph"></d-figure>
    <p>
      <!-- A node, edge and triangle are examples of 1-, 2- and 3-cliques, respectively.  -->
      Factorized representations can be very convenient as they expose structure in a model.
      The desire to visualize this structure motivates <b>factor graphs</b> - both a useful visual representation that explicitly displays the factorization of a distribution and a strong master representation for thinking about problems <d-cite key="dellaert2017factor"></d-cite>.
    </p>
    <p>
      A factor graph is a type of graphical model with circles for variable nodes, squares for factors, and edges connecting each factor to the variables it depends on.
      An example of a simple factor graph is shown in the <a href="#factor_graph">diagram</a> on the right.
      By explicitly having the factors as nodes in the graph, factor graphs simply represent the conditional independence structure 
      - the lack of a factor directly connecting two variables means they are conditionally independent 
      <d-footnote>
        Mathematically, two variables $x_i$ and $x_j$ are conditionally independent given all other variables $X_{-ij}$ if:
        <d-math block="">
          p(x_i, x_j | X_{-ij}) = p(x_i | X_{-ij}) p(x_j | X_{-ij})
          ~.
        </d-math>
        An equivalent way of writing this definition is: 
        <d-math block="">
          p(x_i | x_j, X_{-ij}) = p(x_i | X_{-ij}) 
          ~.
        </d-math>
        Intuitively, if $X_{-ij}$ causes both $x_i$ and $x_j$, then if we know $X_{-ij}$ we don't need to know about $x_i$ to predict $x_j$ or about $x_j$ to predict $x_i$.
        Conditional independence is often written in shorthand as: $x_i \bot x_j | X_{-ij}$.
      </d-footnote>. 
    </p>
    <p>
      Factor graphs can also be viewed as constraint graphs or energy based models <d-cite key="lecun2006:EBM"></d-cite> where each factor $f_i$ captures an energy $E_i \geq 0$ associated with a subset of the variables $X_i$ 
      <d-footnote>
        This formalism is closely related to the Boltzmann distribution in statistical physics which gives the probability of a state $i$ as a function of the energy of the state and the temperature of the system:
        <d-math block="">
          p_i = \frac{e^{-E_i / k T}}{ \sum_j e^{-E_j / k T}}
          ~,
        </d-math>
        where $k$ is the Boltzmann constant, $T$ is the temperature of the system and $j$ sums over all available states.
      </d-footnote>:

      <d-math block="">
        f_i(X_i)
        \propto
        e^{ - E_i(X_i)}
        ~.
      </d-math>

      For example, in our <a href="#factor_graph">diagram</a>, the first factor might represent our prior belief that $x_1$ should be close to some value $x_p$:
      
      <d-math block="">
        E_1(x_1)
        =
        \frac{1}{2} (x_1-x_p)^{\top} \Sigma_1^{-1} (x_1-x_p)
        ~.
      </d-math>

      Alternatively, the factor connecting $x_1$ and $x_2$ might encourage them to have some difference $d$, where $d$ is an observed variable and not shown in the graph:
      
      <d-math block="">
        E_3(x_1, x_2)
        =
        \frac{1}{2} 
        (x_1 - x_2 - d)^\top
        \Sigma_3^{-1}
        (x_1 - x_2 - d)
        ~.
      </d-math>
    </p>
    </g>
    
    <h3 id="the-belief-propagation-algorithm">The Belief Propagation Algorithm</h2>

      <p>
        <!-- To recap, in probabilistic inference we would like to find the marginal posterior distribution for each variable. -->
        Belief propagation (BP) is an algorithm for marginal inference, i.e. it computes the marginal posterior distribution for each variable. 
        BP is intimately linked to factor graphs by the following property: 
        <b>BP can be implemented as iterative message passing on the posterior factor graph</b>. 
      </p>
      <p>
        To draw the posterior factor graph, we need to identify the factors that make up the posterior distribution. 
        Bayes rule tells us exactly this and the factors are the terms on the right hand side of Bayes Rule:  $p(X \rvert D) = \frac{p(D \rvert X) p(X)}{p(D)}$. 
      </p>

      <p>
        Belief propagation was originally developed to perform exact inference in probabilistic models with tree-like
        <d-footnote>
          A <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)#:~:text=In%20graph%20theory%2C%20a%20tree,a%20connected%20acyclic%20undirected%20graph." target="_">tree</a> is a graph in which any two nodes are connected by exactly one path.
        </d-footnote>
        graphical structure <d-cite key="Pearl:book1988"></d-cite>.
        The algorithm operates by storing a belief at each variable node which through iterative message passing converges to the desired marginal distribution.
        Each iteration consists of 3 phases: 
      </p>
      <d-figure id="phases"></d-figure>

      <g>
      <d-figure id="mp_videos" class="right-d-figure">
        <video style="width: 49.5%;" loop controls muted>
          <source src="diagrams/tree_bp.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <video style="width: 49.5%;" loop controls muted>
          <source src="diagrams/loopy_bp.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <figcaption style="text-align: left">
          Green curves represent variable belief distributions. <b>Left</b>: BP on a tree. <b>Right</b>:  BP with synchronous updates applied to a graph with a loop. 
        </figcaption>
      </d-figure>

      <p>
        For trees, after one sweep of messages from a root node to the leaf nodes and then back up, all beliefs converge to the exact marginals <d-cite key="Pearl:book1988"></d-cite>. 
        For models with arbitrary conditional independence structure, including cycles or "loops", loopy BP <d-cite key="Murphy:etal:1999"></d-cite> iteratively applies the same message passing rules to all nodes.
        The simplest variant of loopy BP sends messages from all nodes at every iteration in a synchronous fashion, however other message schedules are available.
        The <a href="#mp_videos">videos</a> on the right illustrate how BP is applied to trees and graphs with loops.
      </p>
      <p>
        Loopy BP generally converges to the true marginals although it can fail to converge for very loopy graphs <d-cite key="Murphy:etal:1999, Weiss:Freeman:NIPS2000, wainwright2008graphical"></d-cite>.  
        This behaviour can be understood by viewing loopy BP as an approximate variational inference method in which inference is cast as the problem of optimizing a free energy. 
        Loopy BP can be derived via constrained minimization of the Bethe free energy which is a non-convex approximation of the variational free energy.
        BP variants have been developed using more accurate approximations of the free energy <d-cite key="yedidia2000generalized"></d-cite>, however a detailed discussion of the theory behind BP is beyond the scope of this article and we refer the reader to <d-cite key="wainwright2008graphical"></d-cite> for a in depth review. 
        Most interesting problems have loopy structures and so for the remainder of the article we will use BP to refer to loopy BP.
      </p>
      </g>
      <p>
        The belief propagation algorithm can be fully defined by the 3 equations in the <a href="#bp_equations">figure</a> below, one for each phase of operation.
      </p>

      <d-figure id="bp_equations" class="subgrid"></d-figure>
    
      <p>
        So far, although we have outlined the BP equations, we have not specified the form of the factors, messages or beliefs. 
        In the remainder of the techical introduction, we focus on Gaussian belief propagation which is a special form of continous BP for Gaussian models. 
      </p>

      <h3 id="gaussian-models">Gaussian Models</h2>

        <p>
          Although in general factors can be arbitrary constraints, we are interested in <b>Gaussian models</b> in which all factors and therefore the joint posterior are univariate / multivariate Gaussian distributions.
          Gaussians are a convenient choice for a number of reasons: (1) they accurately represent the distribution for many real world events <d-cite key="Jaynes:probability2003"></d-cite>, (2) they have a simple analytic form, (3) complex operations can be expressed with simple formulae, and (4) they are closed under marginalization, conditioning and taking products (up to normalization). 
        </p>
    
        <p>
          A Gaussian factor or in general any Gaussian distribution can be written in the exponential form $p(x) \propto e^{-E(x)}$ with a quadratic energy function. 
          There are two ways to write the quadratic energy which correspond to the two common parameterizations of multivariate Gaussian distributions: the <b>moments form</b><d-footnote>It's called the moments form as it is parameterized by the first moment and second central moments of the distribution.</d-footnote> and the <b>canonical form</b>. The key properties of each of these parameterizations are summarized in the <a href="#gaussian_equations">table</a> below. 
        </p>
    
        <d-figure id="gaussian_equations" ></d-figure>
    
        <p>
          During inference, when representing Gaussian posterior distributions we usually use the canonical form for two main reasons.
          First, taking a product is simple in the canonical form so it is easy to form the posterior from the factors, and second the precision matrix is sparse and relates closely to the structure of the factor graph as we explore below. 
        </p>
        <p>
          The precision matrix describes direct associations or conditional dependence between variables. 
          If entry $(i,j)$ of the precision matrix is zero then equivalently, there is no factor that directly connects $x_i$ and $x_j$ in the graph. 
          You can see this in the default preset graph in the <a href="#gaussian_gm">figure</a> below where $\Lambda_{13}=\Lambda_{31}=0$ and $x_1$ and $x_3$ have no factor directly connecting them.
        </p>
        <p>
          On the other hand, the covariance matrix describes induced correlations between variables and is dense as long as the graph is one single connected component. 
          Unlike the canonical form, the moments form is unable to represent unconstrained distributions, as you can see by selecting the unanchored preset graph in which there is only relative positional information. 
          We encourage you to check out the other preset graphs and edit the graph yourself to explore Gaussian models and the relationship with the canonical form.
        </p>
    
        <d-figure id="gaussian_gm" class="subgrid"></d-figure>
    
        <h3 id="from-gaussian-inference-to-linear-algebra">From Gaussian Inference to Linear Algebra</h2>
    
        <p>
          In Bayesian inference, we first form the joint posterior in the canonical form,
          <d-math block="">
            P(X) \propto \exp( - \frac{1}{2} X^\top \Lambda X + \eta^\top X)
            ~,
          </d-math>
          and then compute properties of the posterior such as the MAP parameters or the marginal posteriors.
        </p>
  
        <p>
          <b>MAP inference</b> computes the parameters that maximize the posterior. Clearly for Gaussian models, the maximum is at the mean and therefore $X_{\text{MAP}} = \mu = \Lambda^{-1} \eta$. 
          <!-- Note that maximizing the posterior is also equivalent to minimizing a least squares energy objective: -->
          <!-- <d-math block="">
            X_{\text{MAP}} = \text{arg min}_X  \; - \log p(X) = \text{arg min}_X \; (X - \mu)^\top \Sigma^{-1} (X - \mu) = \mu
          </d-math> -->
        </p>
        <p>
          <b>Marginal inference</b> computes the per-variable marginal posterior distributions.
          In the moments form, the marginal distribution of $x_i$ is:
          <d-math block="">
            p(x_i) = \int p(X) dX_{-i}  \propto \exp\big( -\frac{1}{2}(x_i - \mu_i)^\top \Sigma_{ii}^{-1} (x_i - \mu_i) \big)
            ~,
          </d-math>
          where the mean parameter $\mu_i$ is the $i^{th}$ element of the joint mean vector and the covariance $\Sigma_{ii}$ is entry $(i,i)$ of the joint covariance matrix.
          The vector of marginal means for all variables is therefore the joint mean vector $ \mu = \Lambda^{-1} \eta$ = $X_{\text{MAP}}$ and the marginal varainces are the diagonal entries of the joint covariance matrix $\Sigma = \Lambda^{-1}$.
        </p>
  
        <p>
          We can therefore summarise inference in Gaussian models as solving the linear system of equations $Ax=b \Leftrightarrow
           \Lambda \mu = \eta$. 
          <!-- The precision matrix $\Lambda$ is a real, square, symmetric, positive definite matrix with known sparsity structure. -->
          <b>MAP inference</b> solves for $\mu$ while <b>marginal inference</b> solves for both  $\mu$ and the block diagonal elements of $\Lambda^{-1}$.
        </p>
  
    
        <!-- <d-figure id="probinf_eqns"></d-figure> -->
  
        <!-- 
        <p>
          Directly solving the linear system for $x_{\text{\tiny MAP}}$ by inverting $\Lambda$ gives the uncertainty which is the marginal variances or the diagonal elements of the joint covariance matrix $\Sigma = \Lambda^{-1}$. 
          For large systems, this inversion can be very expensive and Gaussian Belief Propagation is an iterative methods that estimates the full marginal distributions, giving both the MAP estimate (the marginal means) and the uncertainty (the marginal variances). We discuss related linear solvers and non-linear least squares optimisation methods <a href="#related-methods">later on</a>.    
        </p> -->



    <h3 id="gaussian-belief-propagation-ti">Gaussian Belief Propagation</h2>

    <p>
      Having introduced Gaussian models, we now discuss <b>Gaussian Belief Propagation (GBP)</b> a form of BP applied to Gaussian models.
      Due to the closure properties of Gaussians, the beliefs and messages are also Gaussians and GBP operates by storing and passing around information vectors and precision matrices.
    </p>
    <p>
      As is the case for loopy belief propagation, the GBP beliefs converge towards the marginal distributions, however unlike other forms of loopy BP, GBP has theoretical guarantees that it computes the exact marginal means on convergence <d-cite key="Weiss:Freeman:NIPS2000"></d-cite>.
      Unfortunately, the same is not true for the variances and although the variances often converge to the true marginal variances, they are sometimes overconfident for very loopy graphs <d-cite key="Weiss:Freeman:NIPS2000"></d-cite>. 
      Although GBP does not in general have convergence guarantees, there some convergence conditions <d-cite key="Bickson:PhDThesis:2008, du2018convergence, su2015convergence"></d-cite> as well as methods to improve chances of convergence (see chapter 22 in <d-cite key="murphy2012machine"></d-cite>). 
    </p>
    <p>  
      The interactive <a href="#gbp_intuition">figure</a> below aims to build intuition for GBP by exploring the effect of individual messages.
      For easy visualization and interpretation of the beliefs, we examine 3 spatial estimation problems with increasing loopiness: a chain, a loop and a grid.
      Click on a variable node to send messages to its adjacent variables and observe how neighbouring beliefs are updated.
      You will see that GBP converges to the true marginals regardless of the order in which messages are passed.
    </p>

    <d-figure id="gbp_intuition"></d-figure>

    <h2 id="beyond-the-standard-algorithm">Beyond the standard algorithm </h2>

    <p>
      We have introduced Gaussian Belief Propagation in its basic form as a probabilistic inference algorithm for Gaussian estimation problems. However, to solve real practical problems with GBP, we often need a number of extra details and tricks which we discuss in this section.

    </p>

    <h3 id="non-linear-factors">Non-linear factors</h3>

    <p>
      <!-- Requiring all factors to be Gaussians is convenient as we can pass around the Gaussian parameters in the form of information vectors and precision matrices. -->
      Although requiring all factors to be Gaussians is a convenient and generally accurate assumption, most interesting problems involve non-linear relationships which result in non-Gaussian factors. 
      GBP can be extended to handle these problems by linearizing to yield Gaussian approximations of the non-Gaussian factors. 
      To understand this linearization, we first look into how the form of factors is specified. 
    </p>
    <p>
      A factor is usually created given some observed data $d$ that we model as $d \sim h(X) + \epsilon$, where $h$ simulates the data generation process from the subset of variables $X$ <d-footnote>We are overloading $X$ here by using it to denote a subset of the variables for ease of notation.</d-footnote> and $\epsilon \sim \mathcal{N}(0, \Sigma_n)$ is Gaussian noise.
      Rearranging, we see that the residual is Gaussian distributed $d - h(X) \sim \mathcal{N}(0, \Sigma_n)$, allowing us to form the factor with energy: 
    </p>
    
    <d-math block="">
      E(X) = \frac{1}{2}(h(X) - d)^\top \Sigma_n^{-1} (h(X) - d)
      ~.
    </d-math>
    
    <p>
      For linear functions $h(X) = \mathtt{J} X + c$, the energy is quadratic in $X$ and we can rearrange the energy so that the factor is in the Gaussian information form as:
    </p>

    <d-math block="">
      E(X) = \frac{1}{2} X^\top \Lambda X - \eta^\top X
      \;\;\;\;
      \text{, where} \;
      \eta = \mathtt{J}^\top \Sigma_n^{-1} (d - c) 
      \; \text{and} \;
      \Lambda = \mathtt{J}^\top \Sigma_n^{-1} \mathtt{J}
      ~.
    </d-math>


    <p>
      If $h$ is non-linear <d-footnote>The function $h$ could be any non-linear function, for example a trained neural network <d-cite key="czarnowski2020deepfactors"></d-cite> or a Gaussian process <d-cite key="mukadam2018continuous"></d-cite>.</d-footnote>, the energy is no longer quadratic in $X$ meaning the factor is not Gaussian-distributed. 
      To restore the Gaussian form, it is standard to use a first-order Taylor expansion about the current estimate $X_{0}$ to approximate the factor as a Gaussian.
    <d-math block="">
      h(X) \approx h(X_{0}) + \mathtt{J} (X - X_{0})
      ~,  
    </d-math>
      Here $\mathtt{J}$ is now the Jacobian matrix and the factor can be written in the same form as above but with $c = h(X_{0}) - \mathtt{J} X_{0}$ <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>
    <p>
      After linearization, the posterior is a Gaussian approximation of the true posterior and inference is performed by successively solving linearized versions of the underlying non-linear problem, as in non-linear least squares optimization. 
    </p>
    <p>
      To see how this linearization works in practice, consider a robot moving in a plane that measures the 2D distance and angle to a landmark also in the plane.
      The current estimates for the position of the robot and landmark are $r_0$ and $l_0$ respectively and the observed measurement is $h(r_0, l_0)$. 
      In the interactive figure below, we show both the true non-linear factor and the Gaussian approximated factor with $r$ held constant at $r_0$. 
    </p>

    <d-figure id="factor_linearisation"></d-figure>

    <p>
      The accuracy of the approximate Gaussian factor depends on the linearity of the function $h$ at the linearization point.
      As $h$ reasonably is smooth, the linear approximation and therefore the Gaussian factor are good approximations close to $l_0$. 
      Further away from $l_0$, the approximation begins to break down and value of the true factor can differ substantially from the Gaussian approximation.
      During optimization, we can avoid this region of poor approximation by relinearizing often.
      As GBP is local, a just-in-time approach to linearization <d-cite key="Ranganathan:etal:IJCAI2007"></d-cite> can be used in which factors are relinearized individually when the current estimate of the adjacent variables strays significantly from the linearization point.
    </p>

    <h3 id="robust-loss-functions">Robust Loss Functions</h3>

    <g>
    <d-figure class="small-right-d-figure" id="huber"></d-figure>

    <p>
      Often due to sensor frailties or unknown hidden variables real datasets contain a large number of outlying observations. 
      The Gaussian distribution has low probability tails and so a model based on Gaussian-distributed observations is strongly influenced by the large error outliers.
      To reduce sensitivity to outliers, there are a large class of robust loss functions or <a href="https://en.wikipedia.org/wiki/M-estimator" target="_blank">M-estimators</a> which model the observed data with a distribution that has greater probability mass in the tails. 
      One example is the Huber loss function <d-cite key="Huber:AMS:1964, Huber:1981"></d-cite> which models the distribution with a Gaussian close to the mean and with a Laplace distribution
      <d-footnote>
        The probability density function for the Laplace distribution is 
        <d-math block="">
          p(x ; \mu, \beta) = \frac{1}{2b} \exp\big(\frac{-|x - \mu|}{b}\big)
          ~.
        </d-math> 
      </d-footnote>
      in the tails; this is equivalent to penalising squared residuals for inliers and absolute residuals for outliers.
    </p>

    <p>
      Robust estimators can be used within the Gaussian framework by rescaling the covariance of a Gaussian to match the loss of the robust estimator at that residual <d-cite key="Agarwal:etal:ICRA2013, Davison:Ortiz:ARXIV2019"></d-cite>. 
      The <a href="#huber">figure</a> on the right displays how the rescaled Gaussian factor is computed.
      Our interpretation is that robust factors play a similar role to non-linearities in neural networks, activating or deactivating messages in the graph.
      Indeed recent works have shown the importance of robust factors with GBP - they are crucial to identify outlying correspondences in bundle adjustment <d-cite key="Ortiz:etal:CVPR2020"></d-cite> and outlying semantic predictions in incremental scene abstraction <span class="note">(fix ref)</span><d-cite key="Ortiz:planes2021"></d-cite>.  
      Robustness to outliers is crucial for many other applications, and we will explore its importance for image denoising in a later <a href="#attentiongl">interactive figure</a>.
    </p>

    </g>

    <h3 id="local-updates-and-scheduling">Local updates and Scheduling</h3>
    
    <p>
      So far, we have assumed that all variable and factor nodes broadcast messages at each iteration in a synchronous fashion, where all nodes absorb and broadcast messages in parallel. 
      In fact, this is far from a requirement and as GBP is entirely local, messages can be sent arbitrarily and asynchronously. 
    </p>

    <p>
      It turns out that choosing the message schedule is an easy way to speed up convergence and there are a number of different approaches. Simply swapping synchronous updates for random message passing tends to improve convergence, while a fixed "round-robin" schedule can do even better <d-cite key="koller2009probabilistic"></d-cite>.
      Better yet, if each message requires some unit of computation (and therefore energy), it's possible to prioritise sending messages that we think will contribute most to the overall convergence of the system (there is evidence that the brain may apply a similar economical principle<d-cite key="Evans:Burgess:NIPS2019"></d-cite>).
      This is the idea behind residual belief propagation (RBP) <d-cite key="Elidan:etal:UAI2006"></d-cite> and similar variants <d-cite key="Sutton:McCallum:UAI2007, Ranganathan:etal:IJCAI2007"></d-cite>, which form a message queue according to the norm of the difference from the previous message.
    </p>

    <p>
      In the <a href="#gbp1d">figure</a> below, we explore different message schedules for 1D line fitting / surface estimation.
      The blue circles and lines show the mean and standard deviation of the belief estimates of the surface height at fixed intervals along the horizontal direction. 
      The red squares are surface measurements that produce data factors in the graph and there are also smoothness factors between all adjacent variable nodes encouraging the surface estimates to be close.
      You can add your own data factors by clicking on the canvas and a diagram of the factor graph is in the bottom right of the <a href="#gbp1d">figure</a>.
    </p>

    <p>
      The underlying factor graph is a chain (no loops) and so will converge after one sweep of messages from left to right and back again.
      You can send messages through the graph using the preset schedules (synchronous, random or sweep) or create your own schedule by clicking on a variable node to send messages outwards.
    </p>

    <d-figure id="gbp1d" class="subgrid"></d-figure>

    <p>
      Playing around with different schedules for surface estimation highlights two important properties of GBP.
      First, GBP can converge with an arbitrary message passing schedule. 
      As a consequence, GBP can readily operate in systems with no global clock and varying local compute budgets such as on neuromorphic hardware or between a group of distributed devices <d-cite key="micusik2020ego"></d-cite>.
    </p>

    <p>
      The second property is that GBP can achieve approximate local convergence without global convergence. 
      This stems from GBP being a <i>factorized computation</i> <d-cite key="diehl2018factorized"></d-cite> method, in which a global problem is solved by jointly solving many interdependent local subproblems.
      As we often do not require a full global solution, GBP can operate in a <b>just-in-time</b> or <b>attention-driven</b> fashion, focusing processing on parts of the graph to solve local subproblems when the task demands. 
      This attention-driven scheduling can be very economical with compute and energy, only sending the most task-critical messages. 
      <!-- Sometimes, if we are only interested in a particular local set of marginals, solving the graph locally without global convergence may be enough to give accurate relative estimates. -->
    </p>


    <p>
      In the <a href="#attentiongl">figure</a> below we explore attention-driven message passing for image denoising. 
      Image denoising is the 2D equivalent of the surface estimation problem from the previous <a href="#gbp1d">figure</a>. 
      The only difference is that previously although variable nodes were at discrete locations the data factors were at any location, while now the data factors are at the same discrete locations as the variable nodes with one per node.
      The figure also revisits the use of robust loss functions with GBP via covariance rescaling which is crucial for sharp denoising. 
    </p>

    <d-figure id="attentiongl" class="subgrid"></d-figure>

    <h3 id="multiscale-learning">Multiscale Learning</h3>

    <p>
      As we have discussed being a local algorithm gives GBP many nice properties, however its locality also has some drawbacks.
      Propagating information from one node to another takes the same number of iterations as the number of hops between the nodes. 
      For nearby nodes in a local region, information can be communicated in a small number of iterations and consensus can be reached quickly, while for distant nodes, a global consensus can take many more iterations to be established. 
      This is an inherent property of local algorithms and can be summarised as low frequency errors decay more slowly than the high frequency errors.
    </p>

    <p>
      Regular grid structured graphs appear a lot in computer vision (e.g. image segmentation) and in discretized boundary value problems (e.g. solving for the temperature profile along a rod).
      Accelerating convergence in such grid graphs has been well-studied in the field of Multigrid methods <d-cite key="briggs2000multigrid"></d-cite>.
      One simple approach is to coarsen the grid which transforms low frequency errors into higher frequency errors that decay faster. 
      After convergence in the coarsened grid, the solution is used to initialise inference in the original grid which now has smaller low frequency errors. 
      This is the idea behind coarse-to-fine optimisation which is broadly used in many grid-based problems where it is simple to build a coarser graph <d-cite key="felzenszwalb2006efficient"></d-cite>. 
    </p>

    <p class="note">
      Redo figure, just as a plot and then redo text. Ref RCN somewhere here?
      In the interactive figure below, we explore coarse-to-fine GBP for aligning a source signal of 54 points with a target signal that is a translated and stretched version of the source signal.
      The task is to estimate the offset along the $x$ direction that maps each source point onto the target line by minimising the residuals (shown in red).
      This is representative of many computer vision tasks such as image stitching, optical flow and stereo disparity estimation.
      You can experiment with the <a href="#coarse_to_fine">figure</a> below to see how coarse-to-fine GBP finds to a better final solution and converges faster.
    </p>

    
    <p>
      Mulitgrid methods can only be applied to graphs with a grid-like structure where it is possible to build coarsened representations the same problem. 
      In general, most problems are more unstructured and it is not clear how to build a coarsened or abstracted representation of the original problem.
      An alternative approach, is to introduce long range connections into the original graph which represent more global features and act as shortcuts for propagating global information.
      This is tackled in <span class="note">(fix ref)<d-cite key="Ortiz:planes2021"></d-cite></span> which adds nodes for planes on top of a point-based SLAM factor graph, allowing global information to propagate more quickly through these higher-level nodes.
    </p>
  
    <h2 id="related-methods">Related Methods</h2>

    <p>
      Solving real non-linear problems with GBP is done by iteratively solving linearized Gaussian versions of the true non-linear problem. 
      This general pattern of successively solving linearized problems underpins many different non-linear inference methods.
      There are efficeint libraries <d-cite key="CeresManual, Dellaert:TechReport2012"></d-cite> for non-linear inference which use trust region methods like Gauss-Newton or line search to guide the repeated linear steps.
    </p>
    <p>
      GBP is just one of many possible algorithms that can be used to solve the linearized Gaussian model.
      To place GBP amongst other related methods, we present an overview of methods for MAP and marginal inference for Gaussian models in the <a href="#related">figure</a> below. 
      As a reminder, inference in Gaussian models is equivalent to solving the linear system $\Lambda \mu = \eta$, for $\mu$ in MAP inference and for $\mu$ and the diagonal elements of $\Lambda^{-1}$ in marginal inference.
      You can hover over the circles in the figure to explore how GBP relates to other methods. 
    </p>

    <d-figure id="related" class="wider"></d-figure>

    <p>
      With so many different inference methods, choosing which method to use can be a challenge in itself. 
      Judging the speed of each method is complex and depends on both the sparsity structure of $\Lambda$ and on the implementation on the available hardware. 
      Our key argument in this article is that we want a general method that is local, probabilistic and iterative which lead us towards Gaussian Belief Propagation.
    </p>
    <p>
      Other notable candidate methods that are local, probabilistic and iterative are Expectation Propagation (EP) <d-cite key="minka2013ep"></d-cite> and Barfoot's algorithm <d-cite key="barfoot2020fundamental"></d-cite>.
      EP is generally not node-wise parallel and simplifies to GBP in the special case when it is node-wise parallel, while Barfoot's algorithm involves extra communication edges and is yet to be applied to real problems. 
      For these reasons GBP stands out as the extreme case that maximizes parallelism and minimizes communication - two principles that are at the core of scalable and low-power computation.
    </p>




    <!--

    <p>
      Disregarding the uncertainty calculation, GBP is solving a non-linear least squares problem to minimize the sum of the factor energies. 
      There are a host of non-linear least squares solvers that can be divided into line search methods and trust region methods.
      Line search methods choose a descent direction and then step size at each iteration.
      Trust region methods approximate the energy using a model within a trust region - in the Gauss-Newton algorithm the model is a quadratic function meaning the factors are approximated as Gaussians as in GBP.
      In trust region methods, the fundamental step involves solving efficient linear solvers that leverage sparsity structure in the symmetric and positive definite information matrix.
    </p>    
    <p>
    Approximate inference algorithms: rejection sampling, monte carlo (Gibbs sampling, metropolis hastings), particle filters, 
    </p> -->

    <h2 id="discussion">Discussion</h2>
  
    <p>
      We envisage that ML systems of the future will be large scale, heterogeneous and distributed and as such will require flexible and scalable probabilistic inference algorithms.
      In this article, we argued that Gaussian Belief Propagation is a strong candidate algorithm as it is local, probabilistic, iterative and asynchronous.
      Additionally, we showed 1) how GBP is much more general with a prescription for handling non-linear factors and robust loss functions, 2) how GBP can operate in an attention-driven fashion and 3) how hierarchical structure can help convergence.
      We hope that this visual introduction will enourage more researchers and practitioners to look into GBP as an alternative to existing inference algorithms.
    </p>

    <p>
      We see many exciting directions for future research into GBP. 
      Some directions we are most excited about are improving theoretical guarantees of GBP, using learned factors <d-cite key="czarnowski2020deepfactors, mukadam2018continuous, opipari2021differentiable"></d-cite>, introducing discrete variables, combining GBP with GNNs <d-cite key="satorras2021neural, kuck2020belief"></d-cite>, using GBP for distributed learning in overparameterized networks and lastly unifying iterative inference with test-time self-supervised learning.
    </p>

    <p class="note">
      As computer vision researchers, we would like to end by drawing a speculative comparison between Gaussian belief propagation and transformers, once again drawing on Sutton's "bitter lesson" <d-cite key="Sutton:BitterLesson2019"></d-cite>. 
      In vision, transformers have recently demonstrated state-of-the-art performance without the inductive biases of CNNs by leveraging massive datasets and training times. In probabilistic inference, much like CNNs, most inference methods achieve fast performance by handcrating the algorithm to exploit specific structure in a problem. This handcrating however means these methods are not scalable and, unlike GBP and transformers, are not general enough to leverage all available compute. We hope therefore, that in the same way that transformers have been scaled to achieve SOTA performance, GBP has the right properties to be arbitrarily scaled and act as a computational glue for future heterogeneous ML systems.
    </p>

    <h2 id="supp-playground">Supplementary Playground</h2>

    <p>
      We encourage the interested reader to explore our supplementary <a href="#playground">GBP playground</a>.
      The playground consists of 2 interative diagrams based around 2D pose graphs.
      In the first you can construct your own pose graph, set the initialization and then choose how messages are passed through the graph.
      The second is a simulation of a robot exploring a 2D environment with landmarks. 
    </p>

    <d-figure id="playground" class="subgrid"></d-figure>


  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      We are grateful to many researchers with whom we have discussed some of the ideas in this paper, especially from the Dyson Robotics Lab and Robot Vision Group at Imperial College London.
      We would particularly like to thank Raluca Scona, Riku Murai, Edgar Sucar, Seth Nabarro, Tristan Laidlow and Stefan Leutenegger.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
