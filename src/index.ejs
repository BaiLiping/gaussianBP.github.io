<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://kit.fontawesome.com/a076d05399.js"></script>
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.5.1.js" integrity="sha256-QWo7LDvxbWT2tbbQ97B53yJnYU3WhH/C8ycbRAkjPDc=" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous"> -->
  
  <link rel="stylesheet" href="dependencies/font-awesome.min.css">
  <script src="dependencies/a076d05399.js"></script>
  <script src="dependencies/template.v2.js"></script>
  <script src="dependencies/d3.v5.min.js"></script>
  <script src="dependencies/jquery-3.5.1.js"></script>
  <script src="dependencies/bootstrap.min.js"></script>
  <link rel="stylesheet" href="dependencies/bootstrap.min.css">
</head>


<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Gaussian Belief Propagation for Spatial AI</h1>
    <p>A visual introduction to Gaussian Belief Propagation for realtime computer vision. </p>
  </d-title>



  <d-article>

    <!-- <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#spatialAI">Spatial AI</a></div>
        <div><a href="#hardware">Hardware</a></div>
        <div><a href="#related">Related Work</a></div>
        <div><a href="#theory">A brief Theoretical Background</a></div>
        <div><a href="#simulations">Simulated Geometric Estimation Problems</a></div>
        <ul>
          <li><a href="#1dsurface">1D Surface Fitting</a></li>
          <li><a href="#2drobot">2D Robot Simulation</a></li>
        </ul>
        <div><a href="#discussion">Discussion</a></div>
      </nav>
    </d-contents> -->


    <h2 id="introduction">Introduction </h2>

    <p>
      The "Bitter Lesson" of machine learning (ML) research has been that "general methods that leverage computation are ultimately the most effective, and by a large margin" (ref. Sutton). At the time of writing, the state-of-the-art across most ML benchmarks is dominated by GPU driven deep learning (ref).
    </p>

    <p>
      The effectiveness of the DL/GPU approach stems largely from the close structural coupling of hardware, algorithm and the underlying task. AlexNet (ref.) is an illustrative example of this principle; natural images have strong local correlations (ref.), so pixel connectivity can be approximated as sparse. This sparsity structure is translation invariant in regular arrays,
    </p>

    <p>
      An exception to this logic is the success of transformer architectures in domains such as NLP (ref.), where this sparsity structure is not present. But this success comes at a cost - modern deep learning models are increasing in size and trained on similarly scaled hardware, a process which is slow and energy intensive (ref.).
    </p>

    <p>
      As ML is increasingly embedded into the physical world, power consumption and communication costs, within and across systems will become limiting factors. Both can be minimized by co-developing algorithm and hardware that minimize "bits x mm" (ref.), by storing data nearer to the location at which it's operated on (ref. Sze).
    </p>

    <p>
      Like the natural world in which they operate, ML systems of the future will also need to be highly inhomogeneous in their representation. General purpose perception systems will represent the world not just at the level of sequences of regular image arrays, but as semantic objects with continuously changing relationships, growing or contracting in size as new data is added or task demands fluctuate.
    </p>

    <h3>The need for probabilistic inference </h3>

    <p>
      We believe that the second core component of future systems, beyond flexibility, will be the ability to represent the world probabilistically. Systems that can take into account the uncertainty in new data relative to the current world model are able to respond more efficiently and robustly to new data meaning less computation and lower energy costs (ref.), or mitigate catastrophic forgetting in continually learning systems (ref.). 
    </p>

    <p>
      Probabilistic representations will also be important where interpretability is desired (ref. Ghahramani) and essential in "Spatial AI" (ref. FM2) applications such as autonomous transport and robotics, which need to act safely in the physical world.
    </p>

    <h3>Gaussian Belief Propagation </h3>

    <p>To support this "hardware jungle" (ref. Sutter) of highly specialized but interconnected systems, we argue for graph-like representations as the root structure. \hl{By building each system on the same core algorithmic framework, communication and....} Probabilistic graphical models (PGM) have a rich research literature in ML, but have not been succesfully scaled in their general undirected (loopy) form (FIG: directed vs. undirected graphs). However, we believe this will change as hardware evolves (ref. Graphcore) and deserves more attention by researchers to further accelerate this shift (ref. Hardware Lottery; Fig: Co-ev.).</p>
    <p>In this article, we discuss Gaussian Belief Propagation (GaBP) as a strong general purpose algorithmic and representational framework for large scale distributed inference. GaBP is a special case of loopy belief propagation (BP), which operates on a per node basis by purely local message passing. This local computation is key to the vision of next-generation AI systems. GaBP, unlike backpropagation, does not require access to a global objective function, making it easily distributable (ref. biological backprop; Fig. ??). GaBP is flexible to changes in graph structure, in that nodes need only to take into account the messages they receive. GaBP can also operate asynchronously, without the need for a global clock. This is especially desirable in larger systems where communication delays become relevant.</p>
    <p>This iterative, asynchronous operation will enable large graphs that are 'always approximately correct', in the sense that local clusters of nodes may be well converged relative to one another, despite misalignment with more distant clusters. This could, for example enable a robot to perform local obstacle avoidance without needing the global graph to have converged. This property is also attractive from an energy consumption perspective; computation can be focused on regions relevant to the current task demand to enable 'just in time' convergence (ref. FM2).</p>
    <p>Unlike general loopy BP, GaBP is convergent under well defined conditions (ref. Bickson). In this article, we will show that GaBP is intimately linked to the essential and ubiquitous problem of solving linear systems of equations <d-math>Ax = b</d-math> (Section 2). We will also explore how introducing non-linear factors can extend the framework for more flexible representation (Section 3.2), how message scheduling affects convergence (Section 3.1) and how communication overheads can be minimized by adding hierarchical structure (Section 3.3). </p>


    <h2 id="technical">Technical Introduction </h2>

    <h3 id="factor_graphs">Factor Graphs </h3>

    <p>
      Factor graphs are bipartite graphical models that consist of variable and factor nodes and are commonly used to represent probability distributions. They capture the factorisation of the distribution by creating nodes for random variables and nodes for the probabilistic constraints between subsets the variables which are the factors that make up the joint distribution. A simple example of a factor graph is shown in the Figure below.
    </p>


    <p>
      In probabilistic inference, factor graphs are a useful tool for visualising the structure of the posterior density. 
      By Bayes rule <d-math>P(x|z) = \frac{P(z|x) P(x)}{P(z)}</d-math>, the posterior density is proportional to the product of the prior on the variables and the likelihood of the observations given the variables.
      Due to the central limit theorem and principle of maximum entropy 
      <d-footnote>If we know the first two moments of the measurement distribution, by the principle of maximum entropy, a Gaussian is the best choice to model the distribution as it does not assume any other information. If we only observe magnitudes, the central limit theorem tells us that the distribution does often follow the Gaussian functional form. \cite{Jaynes} (Give examples in science when we measure first two moments i.e. MSE and when we measure only magnitude e.g. robotics.) </d-footnote>, 
      measurement likelihood distributions and prior distributions are ubiquitously modelled as Gaussians with great practical success. Under this assumption, likelihood factors have the form: 
    </p>

    <d-math block="">
      P(z|x) \propto \exp( - \frac{1}{2} (h(x) - z)^\top \Sigma_m (h(x)-z))
      ~,
    </d-math>

    <p>
      where <d-math>h(x)</d-math> is the generative model or measurement function and \Sigma_m captures the measurement noise model.
      For the remainder of the article we will focus on Gaussian factor graphs in which all factors and therefore the joint density are Gaussian distributions.
    </p>

    <h3 id="gaussian_operations">Operations on Multivariate Gaussians </h2>

    <p>
      Multivariate Gaussian distributions are commonly represented in the moments form: 
    </p>

    <d-math block="">
      P(x) = \frac{1}{\sqrt{(2 \pi)^k |\Sigma_n|}} \exp{(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu))}
    </d-math>

    <p>
      which is parameterised by the mean vector $\mu$ and covariance matrix $\Sigma$. Expanding the quadratic term in the exponent, yields the canonical form:
    </p>

    <d-math block="">
      P(x) \propto \exp{(-\frac{1}{2} x^\top \Lambda x + \eta^\top x)}
    </d-math>

    <p>
      which is parameterised by the information vector <d-math>\eta = \Sigma^{-1} \mu</d-math> and precision matrix <d-math>\Lambda = \Sigma^{-1}</d-math>. 
    </p>

    <p>
      The canonical form is useful when considering graphical models as the precision matrix describes direct associations between variables. If entry $i,j$ is zero then $x_i$ and $x_j$ are conditionally independent 
      <d-footnote>Conditional independence means $\Lambda_{i,j} = 0 \;\; \Longleftrightarrow \;\; P(x_i, x_j | X_{-ij}) = P(x_i | X_{-ij}) P(x_j | X_{-ij}) \;\; \Longleftrightarrow \;\; x_i \bot x_j | X_{-ij}$</d-footnote> 
      and there is no factor $\phi(x_i, x_j)$ that directly connects $x_i$ and $x_j$ in the graph. The precision matrix is therefore often sparse and describes the conditional independence structure. 
      In contrast, the covariance matrix describes induced correlations between variables and is dense as long as the graph is only single connected component.
      The interactive figure below helps build intuition for the how the Gaussian parameterisations relate to the the factor graph. When the graph is not anchored, we see that the moments form cannot represent the system as infinite uncertainty can be represented by zero precision but cannot be represented as a covariance. 
    </p>

    <figure class="l-body">
      <d-figure id="gaussian_gm"></d-figure>
      <figcaption>
        Factor graphs and the Gaussian moments form and canonical forms.
      </figcaption>
    </figure>

    <p>
      Different transformations to Gaussians are more easily carried out in different forms 
      <d-footnote>Note that the Gaussian distribution family is closed under marginalisation and conditioning while the product of two Gaussians in general yields an un-normalised Gaussian.</d-footnote>. Taking the product of two Gaussians in the canonical form can be easily computed by summing the information vectors and precision matrices while in the moments form it is complicated and involves matrix inversion. This difference makes sense if we consider that taking a product of Gaussians corresponds to adding new factors and hence new direct associations to the graph. Conditioning is also simple in the canonical form while complicated in the moments form, whereas the opposite is true for marginalisation. The following interactive figure illustrates conditioning and marginalisation of a Gaussian and relates it to the factor graph. 
    </p>


    <figure class="l-body">
      <d-figure id="marg_cond"></d-figure>
      <figcaption>
        Marginalising and conditioning Gaussians.
      </figcaption>
    </figure>

    <h3 id="pi_to_la">From Probabilistic Inference to Linear Algebra </h2>

    <p>Probabilistic inference is the task of computing properties of the posterior distribution. Typically, we would like to find the variable configuration that maximises the joint posterior over all variables. This is known as the mode or MAP and for Gaussian posteriors is equal to the joint mean and marginal means. We would also like to know about the confidence around the MAP for each variable which is described by the marginal variances or the diagonal of the joint covariance matrix. </p>
    <p>To restate the problem of probabilistic inference, we would like to go from a set of low-dimensional Gaussian factors to the marginal mean and covariance for each variable. The factors are low-dimensional Gaussians as they represents local interactions and so can cheaply be converted between the moment and canonical form. One expensive way to reach our goal would be to take the product of all factors one by one in the covariance form and then read off the marginal distributions. Instead it is computationally cheaper to form the full posterior distribution in the canonical form by adding the information vectors and precision matrices from all factors and then compute the MAP and marginal covariances from this joint canonical form Gaussian. </p>

    <p>Finding the MAP from the canonical form is equivalent to both converting to the moments form and solving a linear system of equations:</p>
    


    <d-math block="">
      \max_x P(x) = \max_x \: \exp(-\frac{1}{2} x^\top \Lambda x + \eta^\top x)
    </d-math>

    <d-math block="">
      = \min_x \: \frac{1}{2} x^\top \Lambda x - \eta^\top x = \min_x \: \frac{1}{2}(x - \mu)^\top \Sigma^{-1} (x - \mu)
    </d-math>

    <d-math block="">
      \frac{\partial}{\partial x} (\frac{1}{2} x^\top \Lambda x - \eta^\top x) = 0 \longrightarrow \Lambda x = \eta \longrightarrow x = \Lambda^{-1} \eta
    </d-math>

    <p>This demonstrates the equivalence of the problems: 1) MAP inference with a canonical form Gaussian posterior, 2) converting from canonical to covariance form, 3) linear least squares (where <d-math>Ax=b</d-math> is know as the normal equations) and 4) solving linear system of equations.
    </p>

    <h3 id="gbp_algorithm">The Gaussian Belief Propagation Algorithm </h2>

    <p>
      Belief Propagation is a probabilistic inference algorithm that computes the per-node marginal distributions from the joint distribution by iterative message passing on the factor graph. BP alternates between factor-to-variable message passing and variable-to-factor message passing described by the following equations.
    </p>

    <!-- <figure class="l-body">
      <d-figure id="gbp_equations"></d-figure>
    </figure> -->

    <figure class="eq-grid">

      <div style="grid-row: 1; grid-column: 1;">
        <d-math> ~~~~~\mu_{f_j \rightarrow x_i} </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 2;">
        <d-math>  ~=~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 3;">
        <d-math>  ~~~~~~~~\sum_{X_j \setminus x_i} </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 4;">
        <d-math>  ~~~~f(X_j)~~~~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 5;">
        <d-math> ~~~~\prod_{k, x_k \in n(f_j) \setminus x_i}~~~ </d-math>
      </div>

      <div style="grid-row: 1; grid-column: 6;">
        <d-math> ~~~\mu_{x_k \rightarrow f_j} </d-math>
      </div>

      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 1 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 3 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 4 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 5 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 6 / span 1; "></div>
  
      <figcaption style="grid-row: 3; grid-column: 1; max-width:110px;">
        Factor to variable message
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 3; max-width:145px;">
        Marginalise over all other adjacent variables
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 4; max-width:100px;">
        Factor potential
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 5; max-width:130px;">
        Product over all other adjacent variables
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 6; max-width:85px;">
        Message from variable <d-math>x_k</d-math><br>
      </figcaption>


      <div style="grid-row: 4; grid-column: 1;">
        <d-math> ~~~~~\mu_{x_i \rightarrow f_j} </d-math>
      </div>
      <div style="grid-row: 4; grid-column: 2;">
        <d-math>  ~=~~~ </d-math>
      </div>
      <div style="grid-row: 4; grid-column: 3;">
        <d-math> ~~~~\prod_{s, f_s \in n(x_i) \setminus f_j}~~~ </d-math>
      </div>

      <div style="grid-row: 4; grid-column: 4;">
        <d-math> ~~~\mu_{f_s \rightarrow x_i} </d-math>
      </div>

      <div class="expansion-marker-below" style="grid-row: 5; grid-column: 1 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 5; grid-column: 3 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 5; grid-column: 4 / span 1; "></div>
  
      <figcaption style="grid-row: 6; grid-column: 1; max-width:110px;">
        Variable to factor message
      </figcaption>
      <figcaption style="grid-row: 6; grid-column: 3; max-width:140px;">
        Product over all other adjacent factors
      </figcaption>
      <figcaption style="grid-row: 6; grid-column: 4; max-width:85px;">
        Message from factor <d-math>f_s</d-math><br>
      </figcaption>

    </figure>

    <p>
      Note that for continuous variables the summation becomes an integral. The per-node beliefs which estimate the marginals can be calculated at any point using the factor-to-variable messages.
    </p>
    
    <figure class="eq-grid">

      <div style="grid-row: 1; grid-column: 1;">
        <d-math> ~~~~p(x_i)~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 2;">
        <d-math> ~~~=~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 3;">
        <d-math> ~~~\prod_{s \in n(x_i)}~~~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 4;">
        <d-math> ~~\mu_{f_s \rightarrow x_i} </d-math>
      </div>

      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 1 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 3 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 4 / span 1; "></div>
  

      <figcaption style="grid-row: 3; grid-column: 1; max-width:85px;">
        Marginal distribution
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 3; max-width:100px;">
        Product over adjacent factors
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 4; max-width:85px;">
        Message from factor <d-math>f_s</d-math><br>
      </figcaption>

    </figure>

    <p>
      When all factors are Gaussians, the messages, beliefs and factors are represented in the canonical form as the most common operation is taking products. The belief estimates the marginals in the canonical form which can then cheaply be converted into the moments form to give the desired marginal means and variances. As GBP proceeds, messages are passed through the graph and the beliefs converge towards the marginal distributions. The following interactive figure presents an overview of GBP for a simple grid alignment problem. 
    </p>

    <figure class="l-body">
      <d-figure id="message_passing"></d-figure>
      <figcaption>
        GBP message passing.
      </figcaption>
    </figure>


    <h2 id="beyond">Beyond the standard algorithm </h2>


    <h3 id="1dsurface">1D Surface estimation</h3>

    <figure class="l-page-outset">
      <d-figure id="surface_fitting"></d-figure>
    </figure>

    <h3 id="1dsurface">GBP Playground</h3>

    These are essentially different types of pose graph estimation problems.

    <figure class="l-page-outset">
      <d-figure id="playground"></d-figure>
    </figure>
  


    <h2 id="discussion">Discussion</h2>
    


  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> 
    </p>

    <p>
      <b>Writing & Diagrams:</b> 
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
