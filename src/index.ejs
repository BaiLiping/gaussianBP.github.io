<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>

  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- <script src="dependencies/twgl.min.js"></script> -->
</head>


<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>A visual introduction to Gaussian Belief Propagation</h1>
    <p>A Framework for Distributed Inference with Emerging Hardware. </p>
  </d-title>



  <d-article>

    <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <ul>
          <li><a href="#the-need-for-probabilistic-inference">The need for probabilistic inference </a></li>
          <li><a href="#gaussian-belief-propagation">Gaussian Belief Propagation</a></li>
        </ul>
        <div><a href="#technical-introduction">Technical Introduction</a></div>
        <ul>
          <li><a href="#probabilistic-inference">Probabilistic Inference</a></li>
          <li><a href="#factor-graphs">Factor Graphs</a></li>
          <li><a href="#the-belief-propagation-algorithm">The Belief Propagation Algorithm</a></li>
          <li><a href="#belief-propagation-with-multivariate-gaussians">Belief Propagation with Multivariate Gaussians</a></li>
          <li><a href="#from-probabilistic-inference-to-linear-algebra">From Probabilistic Inference to Linear Algebra</a></li>
        </ul>
        <div><a href="#beyond-the-standard-algorithm">Beyond the standard algorithm</a></div>
        <ul>
          <li><a href="#non-linear-factors">Non-linear factors</a></li>
          <li><a href="#robust-loss-functions">Robust Loss Functions</li>
          <li><a href="#local-updates-and-scheduling">Local updates and Scheduling</a></li>
          <li><a href="#multiscale-learning">Multiscale Learning</a></li>
        </ul>
        <div><a href="#related-methods">Related Methods</a></div>
        <div><a href="#discussion">Discussion</a></div>
      </nav>
    </d-contents>

    <g>

    <h2 id="introduction">Introduction </h2>

    <p>
      The "Bitter Lesson" of machine learning (ML) research has been that "general methofds that leverage computation are ultimately the most effective, and by a large margin" <d-cite key="Sutton:BitterLesson2019"></d-cite>. At the time of writing, the state-of-the-art across most ML benchmarks is dominated by GPU driven deep learning.
    </p>
    <p>
      The effectiveness of the DL/GPU approach stems largely from the close structural coupling of hardware, algorithm and the underlying task. 
      Convolutional Neural Networks (CNN; <d-cite key="LeCun:etal:IEEE1998"></d-cite>) operate by learning translation invariant spatial filters, which are multiplied over small patches of adjacent pixels, exploiting the fact that correlations in natural images are strongly local. 
      This local structure (also present in general matrix-multiply driven DL) aligns strongly with the structure of GPUs and has meant that the scale and performance of DL has grown with developments in multicore compute.    
    </p>
    <p class="note">
      Talfan - justify hardware algorithm structure. (notes in overleaf)
    </p>
    <p>
      As ML is increasingly embedded into the physical world, power consumption and communication costs both within and across systems will become limiting factors. 
      Both can be minimized by co-developing algorithms and hardware that minimize "bits x millimetres" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>, by storing data nearer to the location at which it's operated on <d-cite key="Sze:Survey2017"></d-cite>.
      Towards this goal, Sutter identifies the emergence of a "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of parallel, heterogeneous, and distributed compute.
    </p>
    <p>
      To operate in the real world, ML systems of the future will also need to be highly inhomogeneous in their representation. 
      General purpose perception systems will represent the world not just at the level of sequences of regular image arrays, but as graphs of semantic objects with continuously changing relationships, growing or contracting in size as new data is added or task demands fluctuate.
    </p>
    <p>
      Stated simply, so far, the inductive biases present in DL, which themselves reflect the task structure, have been well mirrored in GPU hardware. 
      However we see a gradual shift in both hardware and the structure of tasks in AI and believe it is time to think carefully about which algorithms can occupy a similar sweet spot against the backdrop of increasingly heterogeneous compute and representations. 
    </p>
    
    </g>
    
    <h3 id="the-need-for-probabilistic-inference">The need for probabilistic inference </h3>
    <p class="note">
      Talfan - Refer to work on inhomogeneous representations (GNs, Bengio NeurIPS) distributed processing (distributed backrprop). graphnets. Undirected inference: boltzmann machines, EBMs... 
      High level diagram for different structure of problems (sequential, autoregressive, )
    </p>

    <p>
      We believe that the a core component of future systems will be the ability to represent the world probabilistically. 
      Systems that can take into account the relative uncertainty of new data are able to more appropriately update their existing beliefs, learn from less data (meaning less computation and lower energy costs; <span class="note">ref.</span>) and mitigate catastrophic forgetting in continually learning settings <span class="note">(ref.)</span>. 
    </p>
    <p>
      Probabilistic representations will also be important where interpretability is desired <d-cite key="Ghahramani:Nature2015"></d-cite> and essential in "Spatial AI" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> applications such as autonomous transport and robotics, which need to act safely in the physical world.
    </p>

    <h3 id="gaussian-belief-propagation">Gaussian Belief Propagation </h3>

    <p>
      To support this "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of highly specialized but interconnected systems, we argue for probabilistic graphical models (PGM) as the root structure. 
      PGMs have a rich research literature in ML, but have not been successfully scaled in their general undirected (loopy) form <span class="note">(FIG: directed vs. undirected graphs)</span>. 
      However, we believe this will change as hardware <d-cite key="Graphcore"></d-cite> and tasks evolve and deserves more attention by researchers to further accelerate this shift <d-cite key="Hooker:hardware2020"></d-cite> <span class="note">(Fig: Co-ev.)</span>.
    </p>
    <p class="note">
      Talfan - bullet points instead of text here summarising main points:
      Want to be able to handle arbitrarily structured PGMs.
      Want to be able to distribute computation (asynchronous). 
      Want it to be iterative. 
      Want it to be probabilistic. 
      Want convergence guarantees. 

    </p>
    <p>
      In this article, we discuss Gaussian Belief Propagation (GBP) as a strong general purpose algorithmic and representational framework for large scale distributed inference. 
      GBP is a special case of loopy belief propagation (BP), which operates on a per node basis by purely local message passing. 
      Inference by GBP, unlike backpropagation, does not require access to a global objective function, making it easily distributable <span class="note">(ref. biological backprop; Fig. ??)</span>. 
      GBP is flexible to changes in graph structure, in that nodes need only to take into account the messages they receive. GBP can also operate asynchronously, without the need for a global clock. 
      This is especially desirable in large or distributed systems where communication delays become non-negligible relative to clock-speed. 
    </p>
    <p>
      This iterative, asynchronous operation will enable large graphs that are 'always approximately correct', in the sense that local clusters of nodes may be well converged relative to one another, despite misalignment with more distant clusters. This could, for example enable an embodied agent or robot to perform local obstacle avoidance without needing the global graph to have converged. This property is also attractive from an energy consumption perspective; computation can be focused on regions relevant to the current task demand to enable 'just in time' convergence <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>
    <p>
      Unlike general loopy BP, GBP is convergent under well defined conditions <d-cite key="Bickson:PhDThesis:2008"></d-cite>.
      In this article, we will show that GBP is intimately linked to the essential problem of solving linear systems of equations $Ax = b$. 
      We will also explore how introducing non-linear factors can extend the framework for more flexible representation, how message scheduling affects convergence  and how communication overheads can be minimized by adding hierarchical structure. 
    </p>


    <h2 id="technical-introduction">Technical Introduction </h2>

    <h3 id="probabilistic-inference">Probabilistic Inference </h3>

    <p>
      Inference is the problem of estimating statistical properties of an unknown quantity $X$ from known quantities $D$.
      For example, inferring the weather tomorrow (X) from historic data (D) or inferring the 3D structure of an environment (X) from a video sequence (D).
    </p>
    <p>
      The Bayesian method for inference, proceeds by first designing a probabilistic model $p(X, D)$ that describes how all the variables are produced and then using the sum and product rules of probability
      <d-footnote>
      The sum rule is $p(X) = \sum_Y p(X, Y)$ and the product rule is $p(X, Y) = p(Y \rvert X) p(X)$.
      </d-footnote>
      to form the posterior distribution $p(X \rvert D) = \frac{p(X, D)}{p(D)}$.
      The posterior distribution summarises our belief about $X$ after seeing $D$ and can be used for decision making by weighting the cost of each parameter setting with its posterior probability. 
    </p>
    <p>
      From the posterior we can also find things like the most likely configuration of all the variables: $\text{arg max}_X p(X \rvert D)$, the most likely configuration for a single variable: $\text{arg max}_{x_i} \sum_{X \setminus x_i} p(X \rvert D)$, and the uncertainties in $X$.
      From here, we focus on probabilistic inference as the task of computing the marginal posteriors $p(x_i \rvert D) = \sum_{X \setminus x_i} p(X \rvert D)$, from which we can get the most likely configuration and uncertainty for each variable.
    </p>

    <h3 id="factor-graphs">Factor Graphs </h3>

    <p>
      The <a href="https://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem" target="_blank">Hammersley-Clifford theorem</a> tells us that any positive joint distribution can be represented as a product of factors, one per clique, where a clique is a subset of variables in which each variable is connected to all others. 
      <!-- A node, edge and triangle are examples of 1-, 2- and 3-cliques, respectively.  -->
      A factorized represenation can be very convenient to highlight the structure in a model and motivates a very useful visual representation: factor graphs.
    </p>
    <g>
    <d-figure class="right-d-figure" id="factor_graph"></d-figure>
    <p>

      A factor graph is a type of graphical model with circles for variable nodes, squares for factor nodes and edges connecting each factor to the variables it depends on.
      An example of a simple factor graph is shown in the diagram on the right.
      Factor graphs show both the conditional independence structure 
      - the lack of a factor directly connecting two variables means conditional independence <d-footnote>Mathematically, conditional independence means $\Lambda_{i,j} = 0 \;\; \Longleftrightarrow \;\; P(x_i, x_j | X_{-ij}) = P(x_i | X_{-ij}) P(x_j | X_{-ij}) \;\; \Longleftrightarrow \;\; x_i \bot x_j | X_{-ij}$</d-footnote> - and the factorisation of the distribution. 
    </p>
    <p>
      Factor graphs can be viewed as constraint graphs or energy based models where each factor $f_i$ captures an energy $E_i$ associated with a subset of the variables $X_i$:

      <d-math block="">
        f_i(X_i)
        \propto
        e^{ - E_i(X_i)}
        ~.
      </d-math>

      For example, in our diagram, the first factor might represent our prior belief on $x_1$:
      
      <d-math block="">
        E_1(x_1)
        =
        \frac{1}{2} (x_1-x_p)^{\top} \Sigma_1^{-1} (x_1-x_p)
        ~.
      </d-math>

      Alternatively, the factor connecting $x_1$ and $x_2$ might encourage them to have some difference $d$, where $d$ is an observed variable and not shown in the graph:
      
      <d-math block="">
        E_3(x_1, x_2)
        =
        \frac{1}{2} 
        (x_1 - x_2 - d)^\top
        \Sigma_3^{-1}
        (x_1 - x_2 - d)
        ~.
      </d-math>
    </p>
    </g>
    
    <h3 id="the-belief-propagation-algorithm">The Belief Propagation Algorithm</h2>

      <p>
        To recap, in probabilistic inference we would like to find the marginal posterior distribution for each variable.
        Belief propagation (BP) is an algorithm for finding these marginals that is intimately linked to factor graphs by the following property: 
        <b>BP can be implemented as iterative message passing on the posterior factor graph</b>. 
      </p>
      <p>
        To draw the posterior factor graph, we need to identify the factors that make up the posterior distribution. 
        Bayes rule tells us exactly this and the factors are the terms on the right hand side of Bayes Rule:  $p(X \rvert D) = \frac{p(D \rvert X) p(X)}{p(D)}$. 
      </p>

      <p>
        Belief propagation was originally developed to perform exact inference in probabilistic models with tree-like graphical structure <d-cite key="Pearl:book1988"></d-cite>.
        The algorithm operates by storing a belief at each variable node which through iterative message passing converges to the desired marginal distribution.
        In each iteration consists of 3 phases: factor-to-variable message passing, variable-to-factor message passing and belief updates.
      </p>

      <g>
      <d-figure class="right-d-figure">
        <video style="width: 49.5%;" loop controls muted>
          <source src="diagrams/tree_bp.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <video style="width: 49.5%;" loop controls muted>
          <source src="diagrams/loopy_bp.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <figcaption style="text-align: center">
          Green curves represent variable beliefs. <b>Left</b>: BP on a tree. <b>Right</b>:  Loopy BP. 
        </figcaption>
      </d-figure>
      <p>
        For trees, after one sweep of messages from a root node to the leaf nodes and then back up, all beliefs converge to the exact marginals. 
        For models with arbitrary conditional independence structure, including cycles or "loops", loopy BP <d-cite key="Murphy:etal:1999"></d-cite> iteratively applies the same message passing rules to all nodes at the same time.
        The videos on the right illustrate how BP is applied to trees and graphs with loops.
        Loopy BP generally converges to the true marginals although it can fail to converge for very loopy graphs <span class="note">(ref)</span>. 
      </p>

    </g>

      <p>
        Loopy BP is a form of approximate variational inference in which inference is turned into an optimisation problem. 
        Loopy BP can be derived by doing constrained minimisation of the Bethe free energy subject to normalization and marginalization constraints.
        The Bethe free energy is a lower bound on the KL variational objective. 
        <span class="note">(ref). Some note about tighter variational bounds? Kikuchi Generalised BP?</span>
      </p>

      <p>
        The belief propagation algorithm can be fully defined by 3 equations: 2 message passing functions and one update function which aggregates incoming messages at a variable node to estimate the marginal distribution or belief. 
      </p>

      <d-figure id="bp_equations" class="subgrid"></d-figure>

    <h3 id="belief-propagation-with-multivariate-gaussians">Belief Propagation with Multivariate Gaussians</h2>

    <p>
      Having outlined the BP algorithm, we focus on the case where all factors are Gaussian distributions. 
      Gaussians are a convenient choice for a number of reasons: (1) they accurately represents the distribution for many real world events <d-cite key="Jaynes:probability2003"></d-cite>, (2) they have a simple analytic form, (3) complex operations can be expressed with simple formulae, and (4) they are closed under marginalisation and conditioning. 
      Due to the closure, by choosing all factors to be Gaussians, all beliefs and messages are also Gaussians and the algorithm is known as <b>Gaussian Belief Propagation (GBP)</b>. 
    </p>
    <p>
      As in loopy belief propagation, the GBP beliefs converge towards the marginal distributions, however unlike other forms of loopy BP, GBP has theoretical guarantees that on convergence we have reached the exact marginal means <d-cite key="Weiss:Freeman:NIPS2000"></d-cite>. 
      The interactive figure below shows the high level operation of GBP for a simple grid alignment problem - messages are passed iteratively through the graph and the beliefs converge towards the true marginals.
    </p>

    <d-figure id="message_passing"></d-figure>




    <p>
      One further detail is the parameterisation of Gaussians. There are two common parameterisations of multivariate Gaussian distributions: the <i>moments form</i><d-footnote>It's called the moments form as it is parameterised by the first moment and second central moments of the distribution.</d-footnote> and the <i>canonical form</i>. 
    </p>

    <d-figure id="gaussian_equations" ></d-figure>

    <p>
      When representing Gaussians posterior distributions we usually use the canonical form due to the relation between the precision matrix and factor graphs. 
      We explore this relationship with the paragraph and interactive figure below.
    </p>
    <p>
      The precision matrix of the canonical form describes direct associations or conditional dependence between variables. In other words, if entry $i,j$ of the precision matrix is zero then there is no factor that directly connects $x_i$ and $x_j$ in the graph. 
      You can see this in the default preset graph in the figure below where $\Lambda_{13}=\Lambda_{13}=0$ and $x_1$ and $x_3$ have no factor directly connecting them.
      On the other hand, the covariance matrix describes induced correlations between variables and is dense as long as the graph is only single connected component. 
      The covariance form is also unable to represent unconstrained distributions, as you can see by selecting the unanchored preset graph. 
      We encourage you to check out the other preset graphs and create your own graphs below to explore Gaussian models and the relationship with the canonical form.
    </p>

    <d-figure id="gaussian_gm" class="subgrid"></d-figure>

    <h3 id="from-probabilistic-inference-to-linear-algebra">From Probabilistic Inference to Linear Algebra</h2>

    <p>
      For Gaussian models, the modes of the individual marginal posteriors are equal to the mode of the full posterior or in other words the marginal means are equivalent to the maximum a posteriori parameters (MAP).
      Therefore by computing the marginals, GBP is finding the MAP solution and also the uncertainty in each estimate.
      Computing the MAP is equivalent to maximizing the negative log posterior which for Gaussian models is equivalent to least squares, solving the linear system $Ax = b$ and converting from the canonical form to the moments form:
    </p>

    <d-figure id="probinf_eqns"></d-figure>

    <p>
      Directly solving the linear system for $x_{\text{\tiny MAP}}$ by inverting $\Lambda$ gives the uncertainty which is the marginal variances or the diagonal elements of the joint covariance matrix $\Sigma = \Lambda^{-1}$. 
      For large systems, this inversion can be very expensive and Gaussian Belief Propagation is an iterative methods that estimates the full marginal distributions, giving both the MAP estimate (the marginal means) and the uncertainty (the marginal variances). We discuss related linear solvers and non-linear least squares optimisation methods <a href="#related-methods">later on</a>.    
    </p>

    <h2 id="beyond-the-standard-algorithm">Beyond the standard algorithm </h2>

    <p>
      We have introduced Gaussian Belief Propagation in its basic form as a probabilistic inference algorithm for Gaussian estimation problems. However, to solve real practical problems with GBP, we often need a number of extra details and tricks which we discuss in the rest of this section.

    </p>

    <h3 id="non-linear-factors">Non-linear factors</h3>

    <p>
      Requiring all factors to be Gaussians is convenient as we can pass around the Gaussian parameters in the form of information vectors and precision matrices. The Gaussian assumption however is restrictive because most interesting problems involve non-linear relationships which result in non-Gaussian factors. GBP can be extended to handle these problems by linearizing to yield Gaussian approximations of the non-Gaussian factors. 
    </p>
    <p>
      To understand this linearization, first we will first revisit the form of factors. 
      Although in general a factor $f_i(X_i) \propto e^{-E_i(X_i)}$ can have an arbitrary energy function $E_i$, for Gaussian factors the energy function should have a quadratic form:
    </p>

    <d-math block="">
      E_i(X_i) = \frac{1}{2} (X_i - \mu)^{\top} \Sigma_i^{-1} (X_i - \mu)
      ~.
    </d-math>

    <p>
      A factor is usually created given some observed data $d_i$ that we model as $d_i \sim h_i(X_i) + \epsilon$, where $h_i$ simulates the data generation process from the subset of variables $X_i$ and $\epsilon \sim \mathcal{N}(0, \Sigma_i)$ is Gaussian noise.
      Rearranging, we see that the residual is Gaussian distributed $d_i - h_i(X_i) \sim \mathcal{N}(0, \Sigma_i)$, allowing us to form the factor with energy: 
    </p>
    
    <d-math block="">
      E_i(X_i) = \frac{1}{2}(h_i(X_i) - d_i)^\top \Sigma_i^{-1} (h_i(X_i) - d_i)
      ~.
    </d-math>
    
    <p>
      For linear functions $h_i(X_i) = \mathtt{J} X_i + c$, the energy is quadratic in $X_i$ and we can rearrange the energy so that the factor is in the Gaussian information form as:
    </p>

    <d-math block="">
      E_i(X_i) = \frac{1}{2} X_i^\top \Lambda X_i - \eta^\top X_i
      \;\;\;\;
      \text{, where} \;
      \eta = \mathtt{J}^\top \Sigma_i^{-1} (d_i - c) 
      \; \text{and} \;
      \Lambda = \mathtt{J}^\top \Sigma_i^{-1} \mathtt{J}
      ~.
    </d-math>


    <p>
      If the function is non-linear, for example if $h_i$ is a neural network <d-cite key="czarnowski2020deepfactors"></d-cite> or a Gaussian process <d-cite key="mukadam2018continuous"></d-cite>, the factor is no longer a Gaussian over $X_i$. 
      To restore the Gaussian form, it is standard to use a first-order Taylor expansion about the current estimate $X_{i,0}$, $h_i(X_i) \approx h_i(X_{i,0}) + \mathtt{J} (X_i - X_{i,0})$, to approximate the factor as a Gaussian. 
      $\mathtt{J}$ is the Jacobian matrix and the factor can be written in the same form as above but now with $c = h_i(X_{i,0}) - \mathtt{J} X_{i,0}$ <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>
    <p>
      After linearization, the posterior represents a Gaussian approximation of the true posterior and inference is performed by successively solving linearised versions of the underlying non-linear problem, as in non-linear least squares optimisation. 
    </p>
    <p>
      To see how this linearization works in practise, consider a robot moving in a plane that measures the 2D distance and angle to a landmark also in the plane.
      The current estimates for the position of the robot and landmark are $r_0$ and $l_0$ respectively and the observed measurement is $h(r_0, l_0)$. 
      In the interactive figure below, we show both the true nonlinear factor and the Gaussian approximated factor with $r$ held constant at $r_0$. 
    </p>

    <d-figure id="factor_linearisation"></d-figure>


    <p>
      The accuracy of the approximate Gaussian factor depends on the linearity of the function $h$ at the linearization point.
      As $h$ reasonably is smooth, the linear approximation and therefore the Gaussian factor are good approximations close to $l_0$. 
      Further away from $l_0$, the linear approximation begins to break down and value of the true factor can differ substantially from the Gaussian approximation.
      During optimization, we can avoid this region of poor approximation by relinearizing often.
      As GBP is local, a just-in-time approach to linearization <d-cite key="Ranganathan:etal:IJCAI2007"></d-cite> can be used in which factors are relinearized individually when the current estimate of the adjacent variables strays from the linearization point.
    </p>

    <h3 id="robust-loss-functions">Robust Loss Functions</h3>

    <g>
    <d-figure class="small-right-d-figure" id="huber"></d-figure>

    <p>
      Often due to sensor frailties or unknown hidden variables real datasets contain a large number of outlying observations. 
      The Gaussian distribution has low probability tails and so a model based on Gaussian-distributed observations is strongly influenced by the large error outliers.
      To reduce sensitivity to outliers, there exists a large class of robust loss functions or <a href="https://en.wikipedia.org/wiki/M-estimator" target="_blank">M-estimators</a> which model the observed data with a distribution that has greater probability mass in the tails. 
      One example is the Huber loss function <d-cite key="Huber:AMS:1964, Huber:1981"></d-cite> which models the distribution with a Gaussian close to the mean and with a Laplace distribution 
      <d-footnote>The probability density function for the Laplace distribution is $p(x ; \mu, \beta) = \frac{1}{2b} \exp\big(\frac{-|x - \mu|}{b}\big)$. </d-footnote>.
      in the tails; this is equivalent to penalising squared residuals for inliers and absolute residuals for outliers.
    </p>

    <p>
      Robust estimators can be used within the Gaussian framework by rescaling the covariance of a Gaussian to match the loss of the robust estimator at that residual <d-cite key="Agarwal:etal:ICRA2013, Davison:Ortiz:ARXIV2019"></d-cite>. A number of recent works have shown covariance rescaling can effectively identify outliers via GBP <d-cite key="Ortiz:etal:CVPR2020"></d-cite>. 
      Robustness to outliers is crucial for many applications, and we will explore its importance for image denoising in a later <a href="#attentiongl">interactive figure</a>.
    </p>

    <p class="note">
      Could include comment here about captcha paper and balancing competing hypotheses.
    </p>

    </g>

    <h3 id="local-updates-and-scheduling">Local updates and Scheduling</h3>
    
    <p>
      So far, we have assumed that all variable and factor nodes broadcast messages at each iteration in a synchronous fashion, where all nodes absorb and broadcast messages in parallel. 
      In fact, this is far from a requirement and as GBP is entirely local, messages can be sent arbitrarily and asynchronously. 
    </p>

    <p>
      It turns out that choosing the message schedule is an easy way to speed up convergence and there are a number of different approaches. Simply swapping synchronous updates for random message passing tends to improve convergence <d-cite key="koller2009probabilistic"></d-cite>, while a fixed "round-robin" schedule can do even better.
    </p>

    <p>
      Better yet, if each message requires some unit of computation (and therefore energy), it's possible to prioritise sending messages that we think will contribute most to the overall convergence of the system.
      This is the idea behind residual belief propagation (RBP) <d-cite key="Elidan:etal:UAI2006"></d-cite>, which forms a queue of messages to be sent according to the norm of the difference from the previous message.
      Of course, it isn't possible to form this queue without actually computing all the messages and so using a cheap proxy for computing the residuals saves time <d-cite key="Sutton:McCallum:UAI2007"></d-cite>.
      Empirically RBP produces even faster and more likely convergence than a "round-robin" schedule and has also been used to accelerate the construction of spatial maps <d-cite key="Ranganathan:etal:IJCAI2007"></d-cite>.
    </p>

    <p>
      There is some evidence that the brain may apply a similar economical principle to RBP <d-cite key="Evans:Burgess:NIPS2019"></d-cite>. Also much like neural activity, GBP can operate without a global clock meaning in practical terms that GBP can be implemented on neuromorphic hardware or can be the language for communication between distributed agents or devices.
    </p>

    <p>
      More generally, GBP is an example of <i>factorized computation</i> <d-cite key="diehl2018factorized"></d-cite> in which a problem is solved by jointly solving many interdependent subproblems.
      In this light, GBP can operate in a <i>just-in-time</i> or <i>attention-driven</i> fashion, focusing processing on parts of the graph to solve the local subproblems when the task demands. 
      An attention-driven schedule can be as economical as possible with compute and energy, only sending the most task-critical messages. 
      Sometimes, if we are only interested in a particular local set of marginals, solving the graph locally without global convergence may be enough to give accurate relative marginals up to non-local corrections.
    </p>

    <p>
      In the figure below we explore attention-driven message passing for the task of image denoising. The figure also revisits the use of robust estimators with GBP via covariance rescaling which is crucial for sharp denoising. 
    </p>

    <d-figure id="attentiongl" class="subgrid"></d-figure>


    <h3 id="multiscale-learning">Multiscale Learning</h3>

    <p>
      Being a local algorithm gives GBP many nice properties that we have discussed, however its locality also has some drawbacks.
      As communication is purely by local message passing, propagating information from one node to another takes the same number of iterations as the number of hops between the nodes. 
      For nearby nodes in a local region, information is communicated in a small number of iterations and consensus can be reached quickly, while for distant nodes, a global consensus can take many more iterations to be established. 
      Stated simply, for local algorithms the low frequency errors decay more slowly than the high frequency errors.
    </p>

    <p>
      Regular grid structured graphs appear a lot in computer vision (e.g. image denoising) and in boundary value problems (e.g. solving for the temperature profile along a rod).
      Accelerating convergence in such grid graphs has been well-studied in the field of Multigrid methods <d-cite key="briggs2000multigrid"></d-cite>.
      One simple approach is to coarsen the grid which transforms low frequency errors into higher frequency errors that decay faster. 
      After convergence, the coarsened grid is used to initialise inference in the original grid which now has smaller low frequency errors. 
      This is the idea behind coarse-to-fine optimisation which is broadly used in many grid-based problems where it is simple to build a coarser graph <d-cite key="felzenszwalb2006efficient"></d-cite>. 
    </p>

    <p>
      In the interactive figure below, we explore coarse-to-fine GBP for aligning a source signal of 54 points with a target signal that is a translated and stretched version of the source signal.
      The task is to estimate the offset along the $x$ direction that maps each source point onto the target line by minimising the residuals (shown in red).
      This is representative of many computer vision tasks such as image stitching, optical flow and stereo disparity estimation.
      You can experiment with the figure below to see how coarse-to-fine GBP finds to a better final solution and converges faster.
    </p>

    <d-figure id="coarse_to_fine"></d-figure>

    <p>
      Mulitgrid methods can only be applied to graphs with a grid-like structure where it is possible to build coarsened graphs that represent the same problem. 
      In general, most problems are more unstructured and it is not possible to build a coarsened or abstracted representation of the original problem.
      An alternative approach, is to introduce long range connections into the original graph which represent more global features and act as shortcuts for propagating global information.
      This is tackled in (ref) which adds nodes for planes on top of a point-based SLAM factor graph, leading to faster convergence.
    </p>

    <!-- <figure class="l-page-outset">
      <d-figure id="playground"></d-figure>
    </figure> -->
  
    <h2 id="related-methods">Related Methods</h2>

    <p>
      As we explored <a href="#from-probabilistic-inference-to-linear-algebra">above</a>, probabilistic inference for linear Gaussian problems is equivalent to solving the linear system $\Lambda \mu = \eta$ where $\mu$ is the MAP estimate and the diagonal elements of $\Lambda^{-1}$ are the covariances.
      When only the MAP estimate is required, solving by directly inverting $\Lambda$ can be very costly for large systems and there are numerous methods for efficiently solving the system without inverting the Fisher information matrix $\Lambda$. 
      These methods leverage the sparsity structure in the symmetric and positive definite information matrix.
      In the context of spatial estimation problems, where there is often strong sparsity structure in the information matrix, a host of methods have been developed <span class="note">(ref)</span>. 
    </p>

    <p>
    For linear problems, direct methods solve the system exactly in a fixed number of steps. Examples of direct methods are Gaussian elimination and a number of methods that factorize the information matrix (LU, QR and Cholesky factorization). 
    For non-linear problems, these linear solvers are wrapped in an algorithm like Gauss-Newton or Levenberg-Marquardt.
    </p>
    
    <p>
      On the other hand there are indirect or iterative methods that converge to the solution from an initial estimate over many steps. Iterative methods can be preferable for very large systems as they can require less compute or for non-linear systems as they can be relinearized more frequently. 
    Examples of iterative methods are gradient descent, the Jacobi and Gauss-Seidel methods, and conjugate gradient methods. 
    </p>
    
    <p>
      Gaussian Belief Propagation is closely related to these iterative methods, and is in fact equivalent to the Jacobi and Gauss-Seidel methods with the right message scheduling if the variances are ignored (first pointed out by Weiss Freeman 2000). 
    </p>
    
    <p>
      The linear solvers can also be used for full probabilistic inference, as the factorized information matrix can be inverted. 
    For probabilistic inference there are also a host of algorithms, inference is essentially integration: 
    Approximate inference algorithms: rejection sampling, monte carlo (Gibbs sampling, metropolis hastings), particle filters, variational methods (mean field inference, interpretation of GBP as variational method - Bethe free energy).
    Other distributed probabilistic inference algorithms exist (ref Tim Barfoot), has not been shown to work, uses extra memory and computation and not as distributed. 
    </p>

    <h2 id="discussion">Discussion</h2>
  

  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> 
    </p>

    <p>
      <b>Writing & Diagrams:</b> 
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
