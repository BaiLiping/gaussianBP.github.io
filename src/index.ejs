<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>

  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- <script src="dependencies/twgl.min.js"></script> -->
</head>


<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>A visual introduction to Gaussian Belief Propagation</h1>
    <p>A Framework for Distributed Inference with Emerging Hardware. </p>
  </d-title>



  <d-article>

    <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <!-- <ul>
          <li><a href="#the-need-for-probabilistic-inference">The need for probabilistic inference </a></li>
          <li><a href="#gaussian-belief-propagation">Gaussian Belief Propagation</a></li>
        </ul> -->
        <div><a href="#technical-introduction">Technical Introduction</a></div>
        <!-- <ul>
          <li><a href="#probabilistic-inference">Probabilistic Inference</a></li>
          <li><a href="#factor-graphs">Factor Graphs</a></li>
          <li><a href="#the-belief-propagation-algorithm">The Belief Propagation Algorithm</a></li>
          <li><a href="#gaussian-models">Gaussian Models</a></li>
          <li><a href="#from-gaussian-inference-to-linear-algebra">From Gaussian Inference to Linear Algebra</a></li>
          <li><a href="#gaussian-belief-propagation-ti">Gaussian Belief Propagation</a></li>
        </ul> -->
        <div><a href="#beyond-the-standard-algorithm">Beyond the standard algorithm</a></div>
        <ul>
          <li><a href="#non-linear-factors">Non-linear factors</a></li>
          <li><a href="#robust-loss-functions">Robust Loss Functions</li>
          <li><a href="#local-updates-and-scheduling">Local updates and Scheduling</a></li>
          <li><a href="#multiscale-learning">Multiscale Learning</a></li>
        </ul>
        <div><a href="#related-methods">Related Methods</a></div>
        <div><a href="#discussion">Discussion</a></div>
      </nav>
    </d-contents>

    <g>

    <p class="note">
      Begin with abstract like overview of article, so we mention GBP early on.
    </p>

    <h2 id="introduction">Introduction </h2>


    <p class="note">
      Andy - main thought at the moment - there is so much good stuff in here, but  I think we're missing  the mark at the moment with regard to giving the reader an actual  feel for  the   main  element of just how  GBP works  and  what  its character is. I would argue for bringing the GBP playground back, or at least something like that, where the reader can really experiment with click by click GBP updates, robust factors, etc.
    </p>

    <p>
      The "Bitter Lesson" of machine learning (ML) research has been that "general methofds that leverage computation are ultimately the most effective, and by a large margin" <d-cite key="Sutton:BitterLesson2019"></d-cite>. At the time of writing, the state-of-the-art across most ML benchmarks is dominated by GPU driven deep learning.
    </p>
    <p>
      The effectiveness of the DL/GPU approach stems largely from the close structural coupling of hardware, algorithm and the underlying task. 
      Convolutional Neural Networks (CNN; <d-cite key="LeCun:etal:IEEE1998"></d-cite>) operate by learning translation invariant spatial filters, which are multiplied over small patches of adjacent pixels, exploiting the fact that correlations in natural images are strongly local. 
      This local structure (also present in general matrix-multiply driven DL) aligns strongly with the structure of GPUs and has meant that the scale and performance of DL has grown with developments in multicore compute.    
    </p>
    <p class="note">
      Talfan - justify hardware algorithm structure. (notes in overleaf)
    </p>
    <p>
      As ML is increasingly embedded into the physical world, power consumption and communication costs both within and across systems will become limiting factors. 
      Both can be minimized by co-developing algorithms and hardware that minimize "bits x millimetres" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>, by storing data nearer to the location at which it's operated on <d-cite key="Sze:Survey2017"></d-cite>.
      Towards this goal, Sutter identifies the emergence of a "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of parallel, heterogeneous, distributed and asynchronous compute.
    </p>
    <p>
      To operate in the real world, ML systems of the future will also need to be highly inhomogeneous in their representation. 
      General purpose perception systems will represent the world not just at the level of sequences of regular image arrays, but as graphs of semantic objects with continuously changing relationships, growing or contracting in size as new data is added or task demands fluctuate.
    </p>
    <p>
      Stated simply, so far, the inductive biases present in DL, which themselves reflect the task structure, have been well mirrored in GPU hardware. 
      However we see a gradual shift in both hardware and the structure of tasks in AI and believe it is time to think carefully about which algorithms can occupy a similar sweet spot against the backdrop of increasingly heterogeneous compute and representations. 
    </p>
    
    </g>
    
    <h3 id="the-need-for-probabilistic-inference">The need for probabilistic inference </h3>
    <p class="note">
      Talfan - Refer to work on inhomogeneous representations (GNs, Bengio NeurIPS) distributed processing (distributed backrprop). graphnets. Undirected inference: boltzmann machines, EBMs... 
      High level diagram for different structure of problems (sequential, autoregressive, )
    </p>

    <p class="note">
Andy - I  would be  even stronger than saying a core component; probability is the only way we know to build convergent incremental inference systems. Also probability as the glue between systems that need to inferface.
    </p>

    
    <p>
      We believe that the a core component of future systems will be the ability to represent the world probabilistically. 
      Systems that can take into account the relative uncertainty of new data are able to more appropriately update their existing beliefs, learn from less data (meaning less computation and lower energy costs; <span class="note">ref.</span>) and mitigate catastrophic forgetting in continually learning settings <span class="note">(ref.)</span>. 
    </p>
    <p>
      Probabilistic representations will also be important where interpretability is desired <d-cite key="Ghahramani:Nature2015"></d-cite> and essential in "Spatial AI" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> applications such as autonomous transport and robotics, which need to act safely in the physical world.
    </p>

    <h3 id="gaussian-belief-propagation">Gaussian Belief Propagation </h3>

    <p>
      To support this "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of highly specialized but interconnected systems, we argue for probabilistic graphical models (PGM) as the root structure. 
      PGMs have a rich research literature in ML, but have not been successfully scaled in their general undirected (loopy) form <span class="note">(FIG: directed vs. undirected graphs)</span>. 
      However, we believe this will change as hardware <d-cite key="Graphcore"></d-cite> and tasks evolve and deserves more attention by researchers to further accelerate this shift <d-cite key="Hooker:hardware2020"></d-cite> <span class="note">(Fig: Co-ev.)</span>.
    </p>
    <p class="note">
      Talfan - bullet points instead of text here summarising main points:
      Want to be able to handle arbitrarily structured PGMs.
      Want to be able to distribute computation (asynchronous). 
      Want it to be iterative. 
      Want it to be probabilistic. 
      Want convergence guarantees. 

    </p>

    <p class="note">
Andy - takes a long time to get to the first mention of GBP.
    </p>

      
    <p>
      In this article, we discuss Gaussian Belief Propagation (GBP) as a strong general purpose algorithmic and representational framework for large scale distributed inference. 
      GBP is a special case of loopy belief propagation (BP), which operates on a per node basis by purely local message passing. 
      Inference by GBP, unlike backpropagation, does not require access to a global objective function, making it easily distributable <span class="note">(ref. biological backprop; Fig. ??)</span>. 
      GBP is flexible to changes in graph structure, in that nodes need only to take into account the messages they receive. GBP can also operate asynchronously, without the need for a global clock. 
      This is especially desirable in large or distributed systems where communication delays become non-negligible relative to clock-speed. 
    </p>
    <p>
      This iterative, asynchronous operation will enable large graphs that are 'always approximately correct', in the sense that local clusters of nodes may be well converged relative to one another, despite misalignment with more distant clusters. This could, for example enable an embodied agent or robot to perform local obstacle avoidance without needing the global graph to have converged. This property is also attractive from an energy consumption perspective; computation can be focused on regions relevant to the current task demand to enable 'just in time' convergence <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>

   <p class="note">
Andy - stronger - GBP is special; GBP with robust factors and linearisation is much more general than most people would think.
    </p>

    
    <p>
      Unlike general loopy BP, GBP is convergent under well defined conditions <d-cite key="Bickson:PhDThesis:2008"></d-cite>.
      In this article, we will show that GBP is intimately linked to the essential problem of solving linear systems of equations $Ax = b$. 
      We will also explore how introducing non-linear factors can extend the framework for more flexible representation, how message scheduling affects convergence  and how communication overheads can be minimized by adding hierarchical structure. 
    </p>

    <div class="box">
      <ol>
        <li>Local.</li>
        <ul>
          <li>Can work well for arbitrary sparsity structure.</li>
          <li>Can maximally exploit available hardware parallelism i.e. is local <d-footnote>An algorithm that is node-wise parallel is as parallel as possible. This is because that only variables that are conditionally dependent communicate directly.</d-footnote>.</li>
        </ul>
        <li>Probabilistic: Estimates uncertainties i.e. does marginal inference rather than MAP inference.</li>
        <li>Iterative</li>
      </ol>
    </div>

    <h2 id="technical-introduction">Technical Introduction </h2>

    <h3 id="probabilistic-inference">Probabilistic Inference </h3>

    <p>
      Inference is the problem of estimating statistical properties of an unknown quantity $X$ from known quantities $D$.
      For example, inferring the weather tomorrow (X) from historic data (D) or inferring the 3D structure of an environment (X) from a video sequence (D).
    </p>
    <p>
      The Bayesian method for inference, proceeds by first designing a probabilistic model $p(X, D)$ that describes how all the variables are produced and then using the sum and product rules of probability
      <d-footnote>
      The sum rule is $p(X) = \sum_Y p(X, Y)$ and the product rule is $p(X, Y) = p(Y \rvert X) p(X)$. 
      </d-footnote>
      to form the posterior distribution $p(X \rvert D) = \frac{p(X, D)}{p(D)}$.
      The posterior distribution summarises our belief about $X$ after seeing $D$ and can be used for Bayesian decision making or other downstream tasks. 
    </p>
    <p>
      From the posterior, we can compute things like (i) the most likely configuration of all the variables $X_{\text{MAP}} = \text{arg max}_X p(X \rvert D)$ or (ii) the marginal posteriors $p(x_i \rvert D) = \sum_{X \setminus x_i} p(X \rvert D)$ which summarise our belief about each individual variable after seeing $D$. 
      These two calculations are known as <b>MAP inference</b> and <b>marginal inference</b> respectively.
      An important difference to note is that MAP inference produces a point estimate while marginal inference retains information about uncertainty.
    </p>

    <h3 id="factor-graphs">Factor Graphs </h3>

    <p>
      The <a href="https://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem" target="_blank">Hammersley-Clifford theorem</a> tells us that any positive joint distribution $p(X)$ can be represented as a product of factors $f_i$, one per clique, where a clique is a subset of variables $X_i$ in which each variable is connected to all others: 
    </p>
    <d-math block="">
      p(X) = \prod_i f_i(X_i)
      ~.
    </d-math>
    <g>
      <d-figure class="right-d-figure" id="factor_graph"></d-figure>
    <p>
      <!-- A node, edge and triangle are examples of 1-, 2- and 3-cliques, respectively.  -->
      Factorized representations can be very convenient as they expose structure in a model.
      The desire to visualize this structure motivates <b>factor graphs</b> - a useful visual representation that explicitly displays the factorization of a distribution.
    </p>
    <p>
      A factor graph is a type of graphical model with circles for variable nodes, squares for factors, and edges connecting each factor to the variables it depends on.
      An example of a simple factor graph is shown in the <a href="#factor_graph">diagram</a> on the right.
      By explicitly having the factors as nodes in the graph, factor graphs simply represent the conditional independence structure 
      - the lack of a factor directly connecting two variables means they are conditionally independent 
      <d-footnote>
        Mathematically, two variables $x_i$ and $x_j$ are conditionally independent given all other variables $X_{-ij}$ if:
        <d-math block="">
          p(x_i, x_j | X_{-ij}) = p(x_i | X_{-ij}) p(x_j | X_{-ij})
          ~.
        </d-math>
        Conditional independence is often written in shorthand as: $x_i \bot x_j | X_{-ij}$.
      </d-footnote>. 
    </p>
    <p>
      Factor graphs can also be viewed as constraint graphs or energy based models <d-cite key="lecun2006:EBM"></d-cite> where each factor $f_i$ captures an energy $E_i \geq 0$ associated with a subset of the variables $X_i$ <d-footnote>This formalism is closely related to the Boltzmann distribution in statistical physics.</d-footnote>:

      <d-math block="">
        f_i(X_i)
        \propto
        e^{ - E_i(X_i)}
        ~.
      </d-math>

      For example, in our <a href="#factor_graph">diagram</a>, the first factor might represent our prior belief that $x_1$ should be close to some value $x_p$:
      
      <d-math block="">
        E_1(x_1)
        =
        \frac{1}{2} (x_1-x_p)^{\top} \Sigma_1^{-1} (x_1-x_p)
        ~.
      </d-math>

      Alternatively, the factor connecting $x_1$ and $x_2$ might encourage them to have some difference $d$, where $d$ is an observed variable and not shown in the graph:
      
      <d-math block="">
        E_3(x_1, x_2)
        =
        \frac{1}{2} 
        (x_1 - x_2 - d)^\top
        \Sigma_3^{-1}
        (x_1 - x_2 - d)
        ~.
      </d-math>
    </p>
    </g>
    
    <h3 id="the-belief-propagation-algorithm">The Belief Propagation Algorithm</h2>

      <p>
        <!-- To recap, in probabilistic inference we would like to find the marginal posterior distribution for each variable. -->
        Belief propagation (BP) is an algorithm for marginal inference, i.e. it computes the marginal posterior distribution for each variable. 
        BP is intimately linked to factor graphs by the following property: 
        <b>BP can be implemented as iterative message passing on the posterior factor graph</b>. 
      </p>
      <p>
        To draw the posterior factor graph, we need to identify the factors that make up the posterior distribution. 
        Bayes rule tells us exactly this and the factors are the terms on the right hand side of Bayes Rule:  $p(X \rvert D) = \frac{p(D \rvert X) p(X)}{p(D)}$. 
      </p>

      <p>
        Belief propagation was originally developed to perform exact inference in probabilistic models with tree-like
        <d-footnote>
          A <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)#:~:text=In%20graph%20theory%2C%20a%20tree,a%20connected%20acyclic%20undirected%20graph." target="_">tree</a> is a graph in which any two nodes are connected by exactly one path.
        </d-footnote>
        graphical structure <d-cite key="Pearl:book1988"></d-cite>.
        The algorithm operates by storing a belief at each variable node which through iterative message passing converges to the desired marginal distribution.
        Each iteration consists of 3 phases: 
      </p>
      <d-figure id="phases"></d-figure>

      <g>
      <d-figure id="mp_videos" class="right-d-figure">
        <video style="width: 49.5%;" loop controls muted>
          <source src="diagrams/tree_bp.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <video style="width: 49.5%;" loop controls muted>
          <source src="diagrams/loopy_bp.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <figcaption style="text-align: left">
          Green curves represent variable belief distributions. <b>Left</b>: BP on a tree. <b>Right</b>:  BP on a graph with a loop. 
        </figcaption>
      </d-figure>

      <p>
        For trees, after one sweep of messages from a root node to the leaf nodes and then back up, all beliefs converge to the exact marginals <d-cite key="Pearl:book1988"></d-cite>. 
        For models with arbitrary conditional independence structure, including cycles or "loops", loopy BP <d-cite key="Murphy:etal:1999"></d-cite> iteratively applies the same message passing rules to all nodes at the same time.
        The <a href="#mp_videos">videos</a> on the right illustrate how BP is applied to trees and graphs with loops.
      </p>
      <p>
        Loopy BP generally converges to the true marginals although it can fail to converge, in particular for very loopy graphs <d-cite key="Murphy:etal:1999, Weiss:Freeman:NIPS2000, wainwright2008graphical"></d-cite>.  
        This behaviour can be understood by viewing loopy BP as an approximate variational inference method in which inference is cast as the problem of optimizing a free energy. 
        Loopy BP can be derived via constrained minimization of the Bethe free energy which is an approximation of the variational free energy.
        BP variants have been developed using more accurate approximations of the free energy <d-cite key="yedidia2000generalized"></d-cite>, however a detailed discussion of the theory behind BP is beyond the scope of this article and we refer the reader to <d-cite key="wainwright2008graphical"></d-cite> for a in depth review. 
        Most interesting problems have loopy structures and so for the remainder of the article we will use BP to refer to loopy BP.
      </p>
      </g>
      <p>
        The belief propagation algorithm can be fully defined by the 3 equations in the <a href="#bp_equations">figure</a> below, one for each phase of operation.
      </p>

      <d-figure id="bp_equations" class="subgrid"></d-figure>
    
      <p>
        So far, although we have outlined the BP equations, we have not specified the form of the factors, messages or beliefs. 
        In the remainder of the techical introduction, we focus on Gaussian belief propagation which is a special form of continous BP for Gaussian models. 
      </p>

      <h3 id="gaussian-models">Gaussian Models</h2>

        <p>
          Although in general factors can be arbitrary constraints, we are interested in <b>Gaussian models</b> in which all factors and therefore the joint posterior are univariate / multivariate Gaussian distributions.
          Gaussians are a convenient choice for a number of reasons: (1) they accurately represent the distribution for many real world events <d-cite key="Jaynes:probability2003"></d-cite>, (2) they have a simple analytic form, (3) complex operations can be expressed with simple formulae, and (4) they are closed under marginalization, conditioning and taking products (up to normalization). 
        </p>
    
        <p>
          A Gaussian factor or in general any Gaussian distribution can be written in the exponential form $p(x) \propto e^{-E(x)}$ with a quadratic energy function. 
          There are two ways to write the quadratic energy which correspond to the two common parameterizations of multivariate Gaussian distributions: the <b>moments form</b><d-footnote>It's called the moments form as it is parameterized by the first moment and second central moments of the distribution.</d-footnote> and the <b>canonical form</b>. The key properties of each of these parameterizations are summarized in the <a href="#gaussian_equations">table</a> below. 
        </p>
    
        <d-figure id="gaussian_equations" ></d-figure>
    
        <p>
          During inference, when representing Gaussians posterior distributions we usually use the canonical form for two main reasons.
          First, taking a product is simple in the canonical form so it is easy to form the posterior from the factors, and second the precision matrix is sparse and relates closely to factor graphs as we explore below. 
        </p>
        <p>
          The precision matrix describes direct associations or conditional dependence between variables. 
          If entry $(i,j)$ of the precision matrix is zero then equivalently, there is no factor that directly connects $x_i$ and $x_j$ in the graph. 
          You can see this in the default preset graph in the <a href="#gaussian_gm">figure</a> below where $\Lambda_{13}=\Lambda_{31}=0$ and $x_1$ and $x_3$ have no factor directly connecting them.
        </p>
        <p>
          On the other hand, the covariance matrix describes induced correlations between variables and is dense as long as the graph is one single connected component. 
          Unlike the canonical form, the moments form is unable to represent unconstrained distributions, as you can see by selecting the unanchored preset graph in which there is only relative positional information. 
          We encourage you to check out the other preset graphs and create your own graphs below to explore Gaussian models and the relationship with the canonical form.
        </p>
    
        <d-figure id="gaussian_gm" class="subgrid"></d-figure>
    
        <h3 id="from-gaussian-inference-to-linear-algebra">From Gaussian Inference to Linear Algebra</h2>
    
        <p>
          In Bayesian inference, we first form the joint posterior in the canonical form,
          <d-math block="">
            P(X) \propto \exp( - \frac{1}{2} X^\top \Lambda X + \eta^\top X)
            ~,
          </d-math>
          and then compute properties of the posterior such as the MAP parameters or the marginal posteriors.
        </p>
  
        <p>
          <b>MAP inference</b> computes the parameters that maximize the posterior. Clearly for Gaussian models, the maximum is at the mean and therefore $X_{\text{MAP}} = \mu = \Lambda^{-1} \eta$. 
          <!-- Note that maximizing the posterior is also equivalent to minimizing a least squares energy objective: -->
          <!-- <d-math block="">
            X_{\text{MAP}} = \text{arg min}_X  \; - \log p(X) = \text{arg min}_X \; (X - \mu)^\top \Sigma^{-1} (X - \mu) = \mu
          </d-math> -->
        </p>
        <p>
          <b>Marginal inference</b> computes the per-variable marginal posterior distributions.
          In the moments form, the marginal distribution of $x_i$ is:
          <d-math block="">
            p(x_i) = \int p(X) dX_{-i}  \propto \exp\big( -\frac{1}{2}(x_i - \mu_i)^\top \Sigma_{ii}^{-1} (x_i - \mu_i) \big)
            ~,
          </d-math>
          where the mean parameter $\mu_i$ is the $i^{th}$ element of the joint mean vector and the covariance $\Sigma_{ii}$ is entry $(i,i)$ of the joint covariance matrix.
          The vector of marginal means for all variables is therefore the joint mean vector $ \mu = \Lambda^{-1} \eta$ = $X_{\text{MAP}}$ and the marginal varainces are the diagonal entries of the joint covariance matrix $\Sigma = \Lambda^{-1}$.
        </p>
  
        <p>
          We can therefore summarise inference in Gaussian models as solving the linear system of equations $Ax=b \Leftrightarrow
           \Lambda \mu = \eta$. 
          <!-- The precision matrix $\Lambda$ is a real, square, symmetric, positive definite matrix with known sparsity structure. -->
          <b>MAP inference</b> solves for $\mu$ while <b>marginal inference</b> solves for both  $\mu$ and the block diagonal elements of $\Lambda^{-1}$.
        </p>
  
    
        <!-- <d-figure id="probinf_eqns"></d-figure> -->
  
        <!-- 
        <p>
          Directly solving the linear system for $x_{\text{\tiny MAP}}$ by inverting $\Lambda$ gives the uncertainty which is the marginal variances or the diagonal elements of the joint covariance matrix $\Sigma = \Lambda^{-1}$. 
          For large systems, this inversion can be very expensive and Gaussian Belief Propagation is an iterative methods that estimates the full marginal distributions, giving both the MAP estimate (the marginal means) and the uncertainty (the marginal variances). We discuss related linear solvers and non-linear least squares optimisation methods <a href="#related-methods">later on</a>.    
        </p> -->



    <h3 id="gaussian-belief-propagation-ti">Gaussian Belief Propagation</h2>

    <p>
      Having introduced Gaussian models, we now discuss <b>Gaussian Belief Propagation (GBP)</b> a form of BP applied to Gaussian models.
      Due to the closure properties of Gaussians, the beliefs and messages are also Gaussians and GBP operates by storing and passing around information vectors and precision matrices.
    </p>
    <p>
      As is the case for loopy belief propagation, the GBP beliefs converge towards the marginal distributions, however unlike other forms of loopy BP, GBP has theoretical guarantees that it computes the exact marginal means on convergence <d-cite key="Weiss:Freeman:NIPS2000"></d-cite>.
      Unfortunately, the same is not true for the variances and although the variances often converge to the true marginal variances, they are sometimes overconfident for very loopy graphs <d-cite key="Weiss:Freeman:NIPS2000"></d-cite>. 
      Although GBP does not in general have convergence guarantees, there some convergence conditions <d-cite key="Bickson:PhDThesis:2008, du2018convergence, su2015convergence"></d-cite> as well as methods to improve chances of convergence (see chapter 22 in <d-cite key="murphy2012machine"></d-cite>). 
    </p>
    <p>  
      The interactive <a href="#gbp_intuition">figure</a> below explores the operation of GBP for a simple grid alignment problem. We have chosen a spatial estimation problem as it is intuitive to visualize the beliefs, however GBP can just as easily be applied to any non-spatial inference problem. 
      The figure is divided into two parts.
      The first shows the algorithm level operation of GBP: as you progress the slider, messages are passed iteratively through the graph and the beliefs converge towards the true marginals. Part 2 aims to give intuition for the message-level operation of GBP. You can send individual messages through the graph and observe how neighbouring beliefs are updated and that GBP converges to the true marginals regardless of the order in which messages are passed.
    </p>

    <d-figure id="gbp_intuition"></d-figure>

    <h2 id="beyond-the-standard-algorithm">Beyond the standard algorithm </h2>

    <p>
      We have introduced Gaussian Belief Propagation in its basic form as a probabilistic inference algorithm for Gaussian estimation problems. However, to solve real practical problems with GBP, we often need a number of extra details and tricks which we discuss in the rest of this section.

    </p>

    <h3 id="non-linear-factors">Non-linear factors</h3>

    <p>
      <!-- Requiring all factors to be Gaussians is convenient as we can pass around the Gaussian parameters in the form of information vectors and precision matrices. -->
      Although requiring all factors to be Gaussians is a convenient and generally accurate assumption, most interesting problems involve non-linear relationships which result in non-Gaussian factors. 
      GBP can be extended to handle these problems by linearizing to yield Gaussian approximations of the non-Gaussian factors. 
      To understand this linearization, we first look into how the form of factors is specified. 
    </p>
    <p>
      A factor is usually created given some observed data $d$ that we model as $d \sim h(X) + \epsilon$, where $h$ simulates the data generation process from the subset of variables $X$ <d-footnote>We are overloading $X$ here by using it to denote a subset of the variables for ease of notation.</d-footnote> and $\epsilon \sim \mathcal{N}(0, \Sigma_n)$ is Gaussian noise.
      Rearranging, we see that the residual is Gaussian distributed $d - h(X) \sim \mathcal{N}(0, \Sigma_n)$, allowing us to form the factor with energy: 
    </p>
    
    <d-math block="">
      E(X) = \frac{1}{2}(h(X) - d)^\top \Sigma_n^{-1} (h(X) - d)
      ~.
    </d-math>
    
    <p>
      For linear functions $h(X) = \mathtt{J} X + c$, the energy is quadratic in $X$ and we can rearrange the energy so that the factor is in the Gaussian information form as:
    </p>

    <d-math block="">
      E(X) = \frac{1}{2} X^\top \Lambda X - \eta^\top X
      \;\;\;\;
      \text{, where} \;
      \eta = \mathtt{J}^\top \Sigma_n^{-1} (d - c) 
      \; \text{and} \;
      \Lambda = \mathtt{J}^\top \Sigma_n^{-1} \mathtt{J}
      ~.
    </d-math>


    <p>
      If $h$ is non-linear, the energy is no longer quadratic in $X$ meaning the factor is not Gaussian-distributed. 
      To restore the Gaussian form, it is standard to use a first-order Taylor expansion about the current estimate $X_{0}$ to approximate the factor as a Gaussian.
    <d-math block="">
      h(X) \approx h(X_{0}) + \mathtt{J} (X - X_{0})
      ~,  
    </d-math>
      Here $\mathtt{J}$ is now the Jacobian matrix and the factor can be written in the same form as above but with $c = h(X_{0}) - \mathtt{J} X_{0}$ <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>
    <p>
      After linearization, the posterior is a Gaussian approximation of the true posterior and inference is performed by successively solving linearized versions of the underlying non-linear problem, as in non-linear least squares optimization. 
    </p>
    <p>
      To see how this linearization works in practice, consider a robot moving in a plane that measures the 2D distance and angle to a landmark also in the plane.
      The current estimates for the position of the robot and landmark are $r_0$ and $l_0$ respectively and the observed measurement is $h(r_0, l_0)$. 
      In the interactive figure below, we show both the true non-linear factor and the Gaussian approximated factor with $r$ held constant at $r_0$. 
    </p>

    <d-figure id="factor_linearisation"></d-figure>

    <p>
      The accuracy of the approximate Gaussian factor depends on the linearity of the function $h$ at the linearization point.
      As $h$ reasonably is smooth, the linear approximation and therefore the Gaussian factor are good approximations close to $l_0$. 
      Further away from $l_0$, the approximation begins to break down and value of the true factor can differ substantially from the Gaussian approximation.
      During optimization, we can avoid this region of poor approximation by relinearizing often.
      As GBP is local, a just-in-time approach to linearization <d-cite key="Ranganathan:etal:IJCAI2007"></d-cite> can be used in which factors are relinearized individually when the current estimate of the adjacent variables strays significantly from the linearization point.
    </p>

    <p>
      <span class="note">Where to include comment about factor graphs as the master representation?</span>
      for example if $h$ is a pretrained neural network <d-cite key="czarnowski2020deepfactors"></d-cite> or a Gaussian process <d-cite key="mukadam2018continuous"></d-cite>,
    </p>

    <h3 id="robust-loss-functions">Robust Loss Functions</h3>

    <g>
    <d-figure class="small-right-d-figure" id="huber"></d-figure>

    <p>
      Often due to sensor frailties or unknown hidden variables real datasets contain a large number of outlying observations. 
      The Gaussian distribution has low probability tails and so a model based on Gaussian-distributed observations is strongly influenced by the large error outliers.
      To reduce sensitivity to outliers, there are a large class of robust loss functions or <a href="https://en.wikipedia.org/wiki/M-estimator" target="_blank">M-estimators</a> which model the observed data with a distribution that has greater probability mass in the tails. 
      One example is the Huber loss function <d-cite key="Huber:AMS:1964, Huber:1981"></d-cite> which models the distribution with a Gaussian close to the mean and with a Laplace distribution
      <d-footnote>
        The probability density function for the Laplace distribution is 
        <d-math block="">
          p(x ; \mu, \beta) = \frac{1}{2b} \exp\big(\frac{-|x - \mu|}{b}\big)
          ~.
        </d-math> 
      </d-footnote>
      in the tails; this is equivalent to penalising squared residuals for inliers and absolute residuals for outliers.
    </p>

    <p>
      Robust estimators can be used within the Gaussian framework by rescaling the covariance of a Gaussian to match the loss of the robust estimator at that residual <d-cite key="Agarwal:etal:ICRA2013, Davison:Ortiz:ARXIV2019"></d-cite>. 
      The <a href="#huber">figure</a> on the right shows how the rescaled Gaussian factor is computed.
      A number of recent works have shown covariance rescaling can effectively identify outliers via GBP <d-cite key="Ortiz:etal:CVPR2020"></d-cite>. 
      Robustness to outliers is crucial for many applications, and we will explore its importance for image denoising in a later <a href="#attentiongl">interactive figure</a>.
    </p>

    <p class="note">
      Could include comment here about captcha paper and balancing competing hypotheses.
      Robust factors play similar role to non-linearities in neural networks.
    </p>

    </g>

    <h3 id="local-updates-and-scheduling">Local updates and Scheduling</h3>
    
    <p>
      So far, we have assumed that all variable and factor nodes broadcast messages at each iteration in a synchronous fashion, where all nodes absorb and broadcast messages in parallel. 
      In fact, this is far from a requirement and as GBP is entirely local, messages can be sent arbitrarily and asynchronously. 
    </p>

    <p>
      It turns out that choosing the message schedule is an easy way to speed up convergence and there are a number of different approaches. Simply swapping synchronous updates for random message passing tends to improve convergence, while a fixed "round-robin" schedule can do even better <d-cite key="koller2009probabilistic"></d-cite>.
      Better yet, if each message requires some unit of computation (and therefore energy), it's possible to prioritise sending messages that we think will contribute most to the overall convergence of the system.
      This is the idea behind residual belief propagation (RBP) <d-cite key="Elidan:etal:UAI2006"></d-cite> and similar variants <d-cite key="Sutton:McCallum:UAI2007, Ranganathan:etal:IJCAI2007"></d-cite>, which form a message queue according to the norm of the difference from the previous message.
    </p>

    <p>
      In the <a href="#gbp1d">figure</a> below, we explore different message schedules for 1D line fitting / surface estimation.
      The blue circles and lines show the mean and standard deviation of the belief estimates of the surface height at fixed intervals along the horizontal direction. 
      The red squares are surface measurements that produce data factors in the graph and there are also smoothness factors between all adjacent variable nodes encouraging the surface estimates to be close.
      You can add your own data factors by clicking on the canvas and a diagram of the factor graph is in the bottom right of the <a href="#gbp1d">figure</a>.
    </p>

    <p>
      The underlying factor graph is a chain (no loops) and so will converge after one sweep of messages from left to right and back again.
      You can send messages through the graph using the preset schedules (synchronous, random or sweep) or create your own schedule by clicking on a variable node to send messages outwards.
    </p>

    <d-figure id="gbp1d" class="subgrid"></d-figure>

    <p>
      Playing around with different schedules for surface estimation highlights two important properties of GBP.
      First, GBP can converge with an arbitrary message passing schedule. 
      As a consequence, GBP can readily operate in systems with no global clock and varying local compute budgets such as on neuromorphic hardware or between a group of distributed devices <d-cite key="micusik2020ego"></d-cite>.
    </p>

    <p>
      The second property is that GBP can achieve approximate local convergence without global convergence. 
      This stems from GBP being a <i>factorized computation</i> <d-cite key="diehl2018factorized"></d-cite> method, in which a global problem is solved by jointly solving many interdependent local subproblems.
      As we often do not require a full global solution, GBP can operate in a <b>just-in-time</b> or <b>attention-driven</b> fashion, focusing processing on parts of the graph to solve local subproblems when the task demands. 
      This attention-driven scheduling can be very economical with compute and energy, only sending the most task-critical messages. 
      <!-- Sometimes, if we are only interested in a particular local set of marginals, solving the graph locally without global convergence may be enough to give accurate relative estimates. -->
    </p>


    <p>
      In the <a href="#attentiongl">figure</a> below we explore attention-driven message passing for image denoising. 
      Image denoising is the 2D equivalent of the surface estimation problem from the previous <a href="#gbp1d">figure</a>. 
      The only difference is that previously although variable nodes were at discrete locations the data factors were at any location, while now the data factors are at the same discrete locations as the variable nodes with one per node.
      The figure also revisits the use of robust loss functions with GBP via covariance rescaling which is crucial for sharp denoising. 
    </p>

    <d-figure id="attentiongl" class="subgrid"></d-figure>

    <p>
      <span class="note">Talfan - not sure if / where this best fits in:</span>
      There is some evidence that the brain may apply a similar economical principle to RBP <d-cite key="Evans:Burgess:NIPS2019"></d-cite>. 
    </p>

    <h3 id="multiscale-learning">Multiscale Learning</h3>

    <p>
      As we have discussed being a local algorithm gives GBP many nice properties, however its locality also has some drawbacks.
      Propagating information from one node to another takes the same number of iterations as the number of hops between the nodes. 
      For nearby nodes in a local region, information can be communicated in a small number of iterations and consensus can be reached quickly, while for distant nodes, a global consensus can take many more iterations to be established. 
      This is an inherent property of local algorithms and can be summarised as low frequency errors decay more slowly than the high frequency errors.
    </p>

    <p>
      Regular grid structured graphs appear a lot in computer vision (e.g. image segmentation) and in discretized boundary value problems (e.g. solving for the temperature profile along a rod).
      Accelerating convergence in such grid graphs has been well-studied in the field of Multigrid methods <d-cite key="briggs2000multigrid"></d-cite>.
      One simple approach is to coarsen the grid which transforms low frequency errors into higher frequency errors that decay faster. 
      After convergence in the coarsened grid, the solution is used to initialise inference in the original grid which now has smaller low frequency errors. 
      This is the idea behind coarse-to-fine optimisation which is broadly used in many grid-based problems where it is simple to build a coarser graph <d-cite key="felzenszwalb2006efficient"></d-cite>. 
    </p>

    <p>
      In the interactive figure below, we explore coarse-to-fine GBP for aligning a source signal of 54 points with a target signal that is a translated and stretched version of the source signal.
      The task is to estimate the offset along the $x$ direction that maps each source point onto the target line by minimising the residuals (shown in red).
      This is representative of many computer vision tasks such as image stitching, optical flow and stereo disparity estimation.
      You can experiment with the <a href="#coarse_to_fine">figure</a> below to see how coarse-to-fine GBP finds to a better final solution and converges faster.
    </p>


   <p class="note">
Andy - I am still struggling to understand what is happening in this figure... just the essential problem rather than the multiscale.
    </p>

    
    <d-figure id="coarse_to_fine"></d-figure>
    <!-- <d-figure id="coarse_to_fine_v2"></d-figure> -->

    <p>
      Mulitgrid methods can only be applied to graphs with a grid-like structure where it is possible to build coarsened representations the same problem. 
      In general, most problems are more unstructured and it is not clear how to build a coarsened or abstracted representation of the original problem.
      An alternative approach, is to introduce long range connections into the original graph which represent more global features and act as shortcuts for propagating global information.
      This is tackled in <span class="note">fix ref<d-cite key="Ortiz:planes2021"></d-cite></span> which adds nodes for planes on top of a point-based SLAM factor graph, allowing global information to propagate more quickly through these higher-level nodes.
    </p>
  
    <h2 id="related-methods">Related Methods</h2>

    <p>
      Solving practical non-linear problems with GBP is done by successively solving linearized Gaussian versions of the underlying problem. 
      This general pattern of successively solving linearized problems underpins many different non-linear inference methods and there are efficeint libraries <d-cite key="CeresManual, Dellaert:TechReport2012"></d-cite> for non-linear inference which use trust region methods like Gauss-Newton or line search methods to guide the repeated linear steps.
    </p>
    <p>
      GBP is just one of many possible algorithms that can be used for solving the linearized Gaussian model.
      To place GBP amongst other related methods, in the <a href="#related">figure</a> below we present an overview of many of the methods for MAP and marginal inference in Gaussian models. 
      You can hover over the circles in the figure to explore how GBP relates to other methods. 
    </p>

    <d-figure id="related" class="wider"></d-figure>

    <p>
      With so many different inference methods, choosing which method to use can be a challenge in itself. 
      Judging the speed of each method is complex and depends on both the sparsity structure of $\Lambda$ and on the implementation on the available hardware. 
      Our key argument in this article is that we want a general method that is local, probabilistic and iterative which lead us towards Gaussian Belief Propagation.
    </p>
    <p>
      Other notable candidate methods that are local, probabilistic and iterative are Expectation Propagation (EP) <d-cite key="minka2013ep"></d-cite> and Barfoot's algorithm <d-cite key="barfoot2020fundamental"></d-cite>.
      EP is generally not node-wise parallel and simplifies to GBP in the special case when it is node-wise parallel, while Barfoot's algorithm involves extra communication edges and is yet to be applied to real problems. 
      For these reasons GBP stands out as the extreme case that maximizes parallelism and minimizes communication - two principles that are at the core of scalable and low-power computation.
    </p>




    <!--

    <p>
      Disregarding the uncertainty calculation, GBP is solving a non-linear least squares problem to minimize the sum of the factor energies. 
      There are a host of non-linear least squares solvers that can be divided into line search methods and trust region methods.
      Line search methods choose a descent direction and then step size at each iteration.
      Trust region methods approximate the energy using a model within a trust region - in the Gauss-Newton algorithm the model is a quadratic function meaning the factors are approximated as Gaussians as in GBP.
      In trust region methods, the fundamental step involves solving efficient linear solvers that leverage sparsity structure in the symmetric and positive definite information matrix.
    </p>    
    <p>
    Approximate inference algorithms: rejection sampling, monte carlo (Gibbs sampling, metropolis hastings), particle filters, 
    </p> -->

    <h2 id="discussion">Discussion</h2>
  
    <p class="note">
      GBP is the transformer of probabilistic inference.
    </p>

    <d-figure id="playground" class="subgrid"></d-figure>


  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      Raluca Scona, Riku Murai, Edgar Sucar, Seth Nabarro.
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> 
    </p>

    <p>
      <b>Writing & Diagrams:</b> 
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
