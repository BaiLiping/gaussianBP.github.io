<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>

  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- <script src="dependencies/twgl.min.js"></script> -->
</head>


<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>A visual introduction to Gaussian Belief Propagation</h1>
    <p>A Framework for Distributed Inference with Emerging Hardware. </p>
  </d-title>



  <d-article>

    <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <ul>
          <li><a href="#the-need-for-probabilistic-inference">The need for probabilistic inference </a></li>
          <li><a href="#gaussian-belief-propagation">Gaussian Belief Propagation</a></li>
        </ul>
        <div><a href="#technical-introduction">Technical Introduction</a></div>
        <ul>
          <li><a href="#factor-graphs">Factor Graphs</a></li>
          <li><a href="#multivariate-gaussians">Multivariate Gaussians</a></li>
          <li><a href="#from-probabilistic-inference-to-linear-algebra">From Probabilistic Inference to Linear Algebra</a></li>
          <li><a href="#the-gaussian-belief-propagation-algorithm">The Gaussian Belief Propagation Algorithm</a></li>
        </ul>
        <div><a href="#beyond-the-standard-algorithm">Beyond the standard algorithm</a></div>
        <ul>
          <li><a href="#non-linear-factors">Non-linear factors</a></li>
          <li><a href="#local-updates-and-scheduling">Local updates and Scheduling</a></li>
          <li><a href="#multiscale-learning">Multiscale Learning</a></li>
        </ul>
        <div><a href="#related-work">Related Work</a></div>
        <div><a href="#discussion">Discussion</a></div>
      </nav>
    </d-contents>

    <g>

    <h2 id="introduction">Introduction </h2>

    <p>
      The "Bitter Lesson" of machine learning (ML) research has been that "general methods that leverage computation are ultimately the most effective, and by a large margin" <d-cite key="Sutton:BitterLesson2019"></d-cite>. At the time of writing, the state-of-the-art across most ML benchmarks is dominated by GPU driven deep learning.
    </p>
    <p>
      The effectiveness of the DL/GPU approach stems largely from the close structural coupling of hardware, algorithm and the underlying task. 
      Convolutional Neural Networks (CNN; <d-cite key="LeCun:etal:IEEE1998"></d-cite>) operate by learning translation invariant spatial filters, which are multiplied over small patches of adjacent pixels, exploiting the fact that correlations in natural images are strongly local. 
      This local structure (also present in general matrix-multiply driven DL) aligns strongly with the structure of GPUs and has meant that the scale and performance of DL has grown with developments in multicore compute.    
    </p>
    <p>
      As ML is increasingly embedded into the physical world, power consumption and communication costs both within and across systems will become limiting factors. 
      Both can be minimized by co-developing algorithms and hardware that minimize "bits x millimetres" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>, by storing data nearer to the location at which it's operated on <d-cite key="Sze:Survey2017"></d-cite>.
      Towards this goal, Sutter identifies the emergence of a "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of parallel, heterogeneous, and distributed compute.
    </p>
    <p>
      To operate in the real world, ML systems of the future will also need to be highly inhomogeneous in their representation. 
      General purpose perception systems will represent the world not just at the level of sequences of regular image arrays, but as graphs of semantic objects with continuously changing relationships, growing or contracting in size as new data is added or task demands fluctuate.
    </p>
    <p>
      Stated simply, so far, the inductive biases present in DL, which themselves reflect the task structure, have been well mirrored in GPU hardware. 
      However we see a gradual shift in both hardware and the structure of tasks in AI and believe it is time to think carefully about which algorithms can occupy a similar sweet spot against the backdrop of increasingly heterogeneous compute and representations. 
    </p>
    
    </g>
    
    <h3 id="the-need-for-probabilistic-inference">The need for probabilistic inference </h3>

    <p>
      We believe that the a core component of future systems will be the ability to represent the world probabilistically. 
      Systems that can take into account the relative uncertainty of new data are able to more appropriately update their existing beliefs, learn from less data (meaning less computation and lower energy costs; Ref.) and mitigate catastrophic forgetting in continually learning settings (ref.). 
    </p>
    <p>
      Probabilistic representations will also be important where interpretability is desired <d-cite key="Ghahramani:Nature2015"></d-cite> and essential in "Spatial AI" <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> applications such as autonomous transport and robotics, which need to act safely in the physical world.
    </p>

    <h3 id="gaussian-belief-propagation">Gaussian Belief Propagation </h3>

    <p>
      To support this "hardware jungle" <d-cite key="Sutter:Jungle2011"></d-cite> of highly specialized but interconnected systems, we argue for probabilistic graphical models (PGM) as the root structure. 
      PGMs have a rich research literature in ML, but have not been successfully scaled in their general undirected (loopy) form (FIG: directed vs. undirected graphs). 
      However, we believe this will change as hardware <d-cite key="Graphcore"></d-cite> and tasks evolve and deserves more attention by researchers to further accelerate this shift <d-cite key="Hooker:hardware2020"></d-cite> (Fig: Co-ev.).
    </p>
    <p>
      In this article, we discuss Gaussian Belief Propagation (GBP) as a strong general purpose algorithmic and representational framework for large scale distributed inference. 
      GBP is a special case of loopy belief propagation (BP), which operates on a per node basis by purely local message passing. 
      Inference by GBP, unlike backpropagation, does not require access to a global objective function, making it easily distributable (ref. biological backprop; Fig. ??). 
      GBP is flexible to changes in graph structure, in that nodes need only to take into account the messages they receive. GBP can also operate asynchronously, without the need for a global clock. 
      This is especially desirable in large or distributed systems where communication delays become non-negligible relative to clock-speed. 
    </p>
    <p>
      This iterative, asynchronous operation will enable large graphs that are 'always approximately correct', in the sense that local clusters of nodes may be well converged relative to one another, despite misalignment with more distant clusters. This could, for example enable an embodied agent or robot to perform local obstacle avoidance without needing the global graph to have converged. This property is also attractive from an energy consumption perspective; computation can be focused on regions relevant to the current task demand to enable 'just in time' convergence <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>.
    </p>
    <p>
      Unlike general loopy BP, GBP is convergent under well defined conditions <d-cite key="Bickson:PhDThesis:2008"></d-cite>.
      In this article, we will show that GBP is intimately linked to the essential problem of solving linear systems of equations $Ax = b$. 
      We will also explore how introducing non-linear factors can extend the framework for more flexible representation, how message scheduling affects convergence  and how communication overheads can be minimized by adding hierarchical structure. 
    </p>


    <h2 id="technical-introduction">Technical Introduction </h2>

    <p>
      Belief propagation was developed to perform exact inference in probabilistic models with tree-like graphical structure <d-cite key="Pearl:book1988"></d-cite>. 
      Although it will work for models with arbitrary conditional independence structure, including cycles or "loops" <d-cite key="Murphy:etal:1999"></d-cite>, it is most effective when applied to those with non-uniform but sparse dependencies. 
    </p>

    <h3 id="factor-graphs">Factor Graphs </h3>

    <p>
      The Hammersley-Clifford theorem tells us that any positive joint distribution can be represented as a product of factors, one per clique, where a clique is a subset of variables in which each variable is connected to all others. 
      A node, edge and triangle are examples of 1-,2- and 3-cliques, respectively. 
    </p>
    <p>
      <d-figure class="right-d-figure" id="factor_graph"></d-figure>

      This theorem allows us to visualize any distribution as a factor graph. 
      Factor graphs are a useful representation for inspecting problems and simplify belief propagation to message passing on a the graph.
      An example of simple factor graph is shown in the figure on the right. 
      We use circles and squares to represent variable and factor nodes, respectively. 
      Factor graphs show both the conditional independence structure 
      (the lack of an edge means conditional independence 
      <d-footnote>Mathematically, conditional independence means $\Lambda_{i,j} = 0 \;\; \Longleftrightarrow \;\; P(x_i, x_j | X_{-ij}) = P(x_i | X_{-ij}) P(x_j | X_{-ij}) \;\; \Longleftrightarrow \;\; x_i \bot x_j | X_{-ij}$</d-footnote>
      ) and the factorisation of the distribution. 
    </p>
    <p>
      In probabilistic inference, we often represent the posterior density $P(x|z) = \frac{P(z|x) P(x)}{P(z)}$ as a factor graph. Here $x$ is the state we want to estimate and $z$ are observations that depend in some way on the state. 
      The factors in the graph are either likelihoods of an observation given the variables (terms from $P(z|x)$) or priors over the variables (terms from $P(x)$). 
    </p>

    <h3 id="multivariate-gaussians">Multivariate Gaussians</h2>

    <p>
      The Gaussian distribution is ubiquitously used to model events in the world with great practical success because of its statistical properties <d-cite key="Jaynes:probability2003"></d-cite>:
      (1) Gaussians have maximal entropy for a given mean and variance and (2) the sum of many independent random variables tends towards being Gaussian distributed by the Central Limit Theorem.
      <d-footnote>If we only know the first two moments of a distribution, the principle of maximum entropy or applying Occam's Razor tells us to use a Gaussian model while if we only observe magnitudes, the central limit theorem tells us that the Gaussian functional form is still often accurate.</d-footnote>
      .
      In this article, we discuss Gaussian Belief Propagation in which all factors and therefore the joint distribution are modelled as Gaussians.
    </p>
    <p>
      We should briefly mention here that it is well known that Gaussians underestimate the true distribution in the tails for many real events and we will discuss ways to handle this later within GBP in section (nonlinear factors). Also there are formulations of BP that directly handle non-Gaussian distributions <d-cite key="Sudderth:2010nonparametric"></d-cite>, however we focus on GBP for simplicity.
    </p>
    <p>
      There are two common parameterisation of multivariate Gaussian distributions: the <i>moments form</i> <d-footnote>It is called the moments form as it is parameterised by the first moment and second central moments of the distribution.</d-footnote> and the <i>canonical form</i>. 
    </p>

    <d-figure id="gaussian_equations" ></d-figure>

    <p>
      When representing posterior distributions which are multivariate Gaussians we usually use the canonical form due to the relation between the precision matrix and graph that we will explore with the help of the interactive figure below. 
      You can click on the question mark in the top right of the figure for some hints and explanations.
    </p>
    <p>
      The precision matrix of the canonical form describes direct associations or conditional dependence between variables, meaning if entry $i,j$ is zero then there is no factor that directly connects $x_i$ and $x_j$ in the graph. 
      You can see this in the figure where $\Lambda_{13}=\Lambda_{13}=0$ and $x_1$ and $x_3$ have no factor directly connecting them.
      On the other hand, the covariance matrix describes induced correlations between variables and is dense as long as the graph is only single connected component. 
      The covariance form is also unable to represent unconstrained distributions, as you can see by selecting the unanchored configuration in the figure. 
    </p>

    <d-figure id="gaussian_gm" class="subgrid"></d-figure>

    <p>
      Different transformations are more easily carried out in different forms. 
      Taking the product of two Gaussians and conditioning are cheap in the canonical form and expensive in the moments form whereas the opposite is true for marginalisation
      <d-footnote>
        The Gaussian distribution family is closed under marginalisation and conditioning while the product of two Gaussians in general yields an un-normalised Gaussian.
      </d-footnote>.
    </p>

    <h3 id="from-probabilistic-inference-to-linear-algebra">From Probabilistic Inference to Linear Algebra</h2>

    <p>
      In probabilistic inference we would often like to find the variable configuration that maximises the posterior along with the confidence around each variable estimate. The process of finding just the point estimate is called <i>maximum a posteriori</i> (MAP) estimation and for Gaussian posteriors, it can be shown to be equivalent to least squares optimisation, solving the linear system $Ax = b$ and converting from the canonical form to the moments form:
    </p>

    <d-figure id="probinf_eqns"></d-figure>

    <p>
      Directly solving the linear system for $x_{\text{\tiny MAP}}$ by inverting $\Lambda$ gives the confidence which is the marginal variances or the diagonal elements of the joint covariance matrix $\Sigma = \Lambda^{-1}$. 
      For large systems, this inversion can be very expensive and Gaussian Belief Propagation is an iterative methods that estimates the full marginal distributions, giving both the MAP estimate (the marginal means) and the confidence (the marginal variances) with potentially lower computational cost. We discuss related linear solvers and non-linear least squares optimisation methods in Section (rel work).      
    </p>

    <h3 id="the-gaussian-belief-propagation-algorithm">The Gaussian Belief Propagation Algorithm</h2>

    <p>
      To recap, Belief Propagation is a probabilistic inference algorithm that computes the per-node marginal distributions from the joint distribution.
      It does so by iterative message passing on the factor graph, alternating between factor-to-variable and variable-to-factor message passing.
      The algorithm is fully described by 2 types of message passing functions and one update function which aggregates incoming messages at a variable node to estimate the marginal distribution or belief. 
    </p>

    <d-figure id="gbp_equations" class="subgrid"></d-figure>

    <p>
      When all factors are Gaussian, the joint distribution and all beliefs are also Gaussian.
      The belief estimates the marginals in the canonical form which can then cheaply be converted into the moments form to give the desired marginal means and variances. 
      As GBP proceeds, messages are passed through the graph and the beliefs converge towards the marginal distributions. The following interactive figure presents an overview of GBP for a simple grid alignment problem. 
    </p>

    <d-figure id="message_passing"></d-figure>

    <h2 id="beyond-the-standard-algorithm">Beyond the standard algorithm </h2>

    <p>
      We have introduced Gaussian Belief Propagation in its basic form as a probabilistic inference algorithm for Gaussian estimation problems. However, to solve real practical problems with GBP, we often need a number of extra details and tricks which we discuss in the rest of this section.

    </p>

    <h3 id="non-linear-factors">Non-linear factors</h3>

    <p>
      For each independent observation $z$, a factor is added to the graph which encourages the set of variables $x$ that we think are casually related the observation to take values that are deemed likely to have generated the observation.
      To determine the likely values, we use a model that simulates the data generation process and this model is often a deterministic function of the variables plus some centred Gaussian noise with variance $\Sigma_n$: $z = h(x) + \epsilon$, where $\epsilon \sim \mathcal{N}(0, \Sigma_n)$.
      Rearranging this equation, we see that the residual is Gaussian distributed: $z - h(x) \sim \mathcal{N}(0, \Sigma_n)$ and the likelihood of the observation given the variables is therefore: 
    </p>

    <d-math block="">
      P(z|x) = \sqrt{(2 \pi)^k |\Sigma|} \; \exp( - \frac{1}{2} (z - h(x))^\top \Sigma_n^{-1} (z - h(x))) ~,
    </d-math>

    <p>where $k$ is the dimension of the observation $z$.</p>

    <p>
      The factor for each observation is formed by evaluating the likelihood at the observed value $z=z^*$, i.e. the factor $f(x; z^*) \propto P(z = z^*|x)$. 
      Importantly, the factor is a function of $x$ only and $z^*$ is an observed parameter. 
      If the measurement function is linear, i.e. $h(x) = J x + c$, the factor is a Gaussian distribution over $x$ and can be written in the information form as (a derivation can be found in <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite>: 
    </p>

    <d-math block="">
      f(x; z^*) \propto \exp(-\frac{1}{2} x^\top \Lambda_f x + \eta_f^\top x)
      \;\;\; \text{, where} \;
      \eta_f = J^\top \Sigma_n^{-1} z^* 
      \; \text{and} \;
      \Lambda_f = J^\top \Sigma_n^{-1} J
    </d-math>

    <p>
      If the function $h$ is nonlinear, the factor is not a Gaussian over $x$ it is standard practise to use a first-order Taylor expansion about the current estimate to approximate the factor as a Gaussian 
      <d-footnote>For non-linear measurement functions, the factor can be linearized about $x_0$ to take the Gaussian canonical form with $\eta = J^\top \Sigma_n^{-1} [z^* - h(x_{0}) + J x_{0}]$ and $\Lambda = J^\top \Sigma_n^{-1} J$ where $J$ is the Jacobian matrix.</d-footnote>
      (see <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> for more details). 
      After linearization, the posterior represents a Gaussian approximation of the true posterior and inference is performed by successively solving linearised versions of the underlying non-linear problem, as in non-linear least squares optimisation. 
    </p>

    <p>
      To see how this linearization works in practise, we will look at two examples: 1) a robot which makes 2D range and bearing measurements and 2) robust loss functions.  
    </p>

    <p>
      In the first example, a robot moving in a plane measures the 2D distance and angle to a landmark also in the plane.
      The current estimates for the position of the robot and landmark are $r_0$ and $l_0$ respectively and the observed measurement is $h(r_0, l_0)$. 
      In the interactive figure below, we show both the true nonlinear factor and the Gaussian approximated factor with r held constant at $r_0$. 
    </p>
  
    <d-figure id="factor_linearisation"></d-figure>

    <p>
      The accuracy of the approximate Gaussian factor depends on the linearity of the measurement function $h$ at the linearization point.
      As $h$ reasonably is smooth, the linear approximation and therefore the Gaussian factor are good approximations close to $l_0$. 
      Further away from $l_0$, we can see that the linear approximation begins to break down and value of the true factor can differ substantially from the Gaussian approximation.
      During optimization, we avoid this region of poor approximation by relinearizing often.
      As GBP is local, a just-in-time approach to linearization <d-cite key="Ranganathan:etal:IJCAI2007"></d-cite> can be used in which factors are relinearized individually when the current estimate of the adjacent variables strays from the linearization point.
    </p>

    <p>
      <d-figure class="small-right-d-figure" id="huber"></d-figure>

      For the second example, consider that we are observing the outcomes of a random process that produces a large number of outlying events. 
      The Gaussian distribution has low probability tails and so a model based on Gaussian-distributed observations is strongly influenced by the large error outliers.
      To reduce sensitivity to outliers, there exists a large class of robust loss functions or M-estimators which model the random process with a distribution that has greater probability mass in the tails. 
      One example is the Huber loss function <d-cite key="Huber:AMS:1964, Huber:1981"></d-cite> (plotted on the right) which models the distribution as a Gaussian close to the mean and a Laplace distribution in the tails; in maximum likelihood estimation it is equivalent to penalising squared errors for inliers and weighted absolute errors for outliers
      <d-footnote>The probability density function for the Laplace distribution is $p(x ; \mu, \beta) = \frac{1}{2b} \exp\big(\frac{-|x - \mu|}{b}\big)$. </d-footnote>.
    </p>

    <p>
      Robust estimators can be used approximately within the Gaussian framework by using similar ideas to dealing with non-linear measurement functions.
      For example, to approximate the Laplace distribution, we can change the measurement function to take as input the observation and instead generate a residual as: $h^\prime(x) = \sqrt{z^* - h(x)}$.
      Linearizing this measurement function will give a Gaussian factor that approximates the true Laplacian factor locally.
    </p>

    <p>
      Covariance rescaling is a simpler and more general alternative to altering the measurement function and has recently been successfully used with GBP <d-cite key="Agarwal:etal:ICRA2013, Davison:Ortiz:ARXIV2019"></d-cite>. 
      Covariance rescaling, does exactly what it says on the tin and rescales the covariance of the factor such that the robust distribution matches up exactly with the approximate Gaussian distribution at the observed value. 
      Robustness to outliers is crucial for many applications, and you can explore its importance for image denoising in this <a href="#attentiongl">interactive figure</a>.
    </p>

    <h3 id="local-updates-and-scheduling">Local updates and Scheduling</h3>
    
    <p>
      So far, we have assumed that all variable and factor nodes broadcast messages at each iteration in a synchronous fashion, where all nodes absorb and broadcast messages in parallel. 
      In fact, this is far from a requirement and as GBP is entirely local, messages can be sent arbitrarily and asynchronously. 
    </p>

    <p>
      It turns out that choosing the message schedule is an easy way to speed up convergence and there are a number of different approaches. Simply swapping synchronous updates for random message passing tends to improve convergence <d-cite key="koller2009probabilistic"></d-cite>, while a fixed "round-robin" schedule can do even better.
    </p>

    <p>
      Better yet, if each message requires some unit of computation (and therefore energy), it's possible to prioritise sending messages that we think will contribute most to the overall convergence of the system.
      This is the idea behind residual belief propagation (RBP) <d-cite key="Elidan:etal:UAI2006"></d-cite>, which forms a queue of messages to be sent according to the norm of the difference from the previous message.
      Of course, it isn't possible to form this queue without actually computing all the messages and so using a cheap proxy for computing the residuals saves time <d-cite key="Sutton:McCallum:UAI2007"></d-cite>.
      Empirically RBP produces even faster and more likely convergence than a "round-robin" schedule and has also been used to accelerate the construction of spatial maps <d-cite key="Ranganathan:etal:IJCAI2007"></d-cite>.
    </p>

    <p>
      There is some evidence that the brain may apply a similar economical principle to RBP <d-cite key="Evans:Burgess:NIPS2019"></d-cite>. Also much like neural activity, GBP can operate without a global clock meaning in practical terms that GBP can be implemented on neuromorphic hardware or can be the language for communication between distributed agents or devices.
    </p>

    <p>
      More generally, GBP is an example of <i>factorized computation</i> <d-cite key="diehl2018factorized"></d-cite> in which a problem is solved by jointly solving many interdependent subproblems.
      In this light, GBP can operate in a <i>just-in-time</i> or <i>attention-driven</i> fashion, focusing processing on parts of the graph to solve the local subproblems when the task demands. 
      An attention-driven schedule can be as economical as possible with compute and energy, only sending the most task-critical messages. 
      Sometimes, if we are only interested in a particular local set of marginals, solving the graph locally without global convergence may be enough to give accurate relative marginals up to non-local corrections.
    </p>

    <p>
      In the figure below we explore attention-driven message passing for the task of image denoising. The figure also revisits the use of robust estimators with GBP via covariance rescaling which is crucial for sharp denoising. 
    </p>

    <d-figure id="attentiongl" class="subgrid"></d-figure>


    <h3 id="multiscale-learning">Multiscale Learning</h3>

    <figure class="l-page-outset">
      <d-figure id="playground"></d-figure>
    </figure>
  
    <h2 id="related-work">Related work</h2>

    <p>
      As we have already explored (href sec from PI to LA), probabilistic inference for linear estimation problems is equivalent to solving the linear system $\Lambda \mu = \eta$ where $\mu$ is the MAP estimate and the diagonal elements of $\Lambda^{-1}$ are the respective confidences.
      Solving by directly inverting $\Lambda$ is very costly for large systems and when only the MAP estimate is required, there are numerous methods for efficiently solving the system without inverting the Fisher information matrix $\Lambda$ which leverage the fact that the information matrix is symmetric and positive definite and that it has strong sparsity.
      In the context of spatial estimation problems, where there is often strong sparsity structure in the information matrix, a host of methods have been developed. 
    </p>

    <p>
    For linear problems, direct methods solve the system exactly in a fixed number of steps. Examples of direct methods are Gaussian elimination and a number of methods that factorize the information matrix (LU, QR and Cholesky factorization). 
    For non-linear problems, these linear solvers are wrapped in an algorithm like Gauss-Newton or Levenberg-Marquardt.
    </p>
    
    <p>
      On the other hand there are indirect or iterative methods that converge to the solution from an initial estimate over many steps. Iterative methods can be preferable for very large systems as they can require less compute or for non-linear systems as they can be relinearized more frequently. 
    Examples of iterative methods are gradient descent, the Jacobi and Gauss-Seidel methods, and conjugate gradient methods. 
    </p>
    
    <p>
      Gaussian Belief Propagation is closely related to these iterative methods, and is in fact equivalent to the Jacobi and Gauss-Seidel methods with the right message scheduling if the variances are ignored (first pointed out by Weiss Freeman 2000). 
    </p>
    
    <p>
      The linear solvers can also be used for full probabilistic inference, as the factorized information matrix can be inverted. 
    For probabilistic inference there are also a host of algorithms, inference is essentially integration: 
    Approximate inference algorithms: rejection sampling, monte carlo (Gibbs sampling, metropolis hastings), particle filters, variational methods (mean field inference, interpretation of GBP as variational method - Bethe free energy).
    Other distributed probabilistic inference algorithms exist (ref Tim Barfoot), has not been shown to work, uses extra memory and computation and not as distributed. 
    </p>

    <h2 id="discussion">Discussion</h2>
  

  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> 
    </p>

    <p>
      <b>Writing & Diagrams:</b> 
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
