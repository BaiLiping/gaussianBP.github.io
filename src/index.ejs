<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://kit.fontawesome.com/a076d05399.js"></script>
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.5.1.js" integrity="sha256-QWo7LDvxbWT2tbbQ97B53yJnYU3WhH/C8ycbRAkjPDc=" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous"> -->
  
  <link rel="stylesheet" href="dependencies/font-awesome.min.css">
  <script src="dependencies/a076d05399.js"></script>
  <script src="dependencies/template.v2.js"></script>
  <script src="dependencies/d3.v5.min.js"></script>
  <script src="dependencies/jquery-3.5.1.js"></script>
  <script src="dependencies/bootstrap.min.js"></script>
  <link rel="stylesheet" href="dependencies/bootstrap.min.css">
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Gaussian Belief Propagation for Spatial AI</h1>
    <p>A visual introduction to Gaussian Belief Propagation for realtime computer vision. </p>
  </d-title>

  <figure class="l-page-outset">
    <d-figure id="robot"></d-figure>
  </figure>


  <d-article>

    <!-- <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#spatialAI">Spatial AI</a></div>
        <div><a href="#hardware">Hardware</a></div>
        <div><a href="#related">Related Work</a></div>
        <div><a href="#theory">A brief Theoretical Background</a></div>
        <div><a href="#simulations">Simulated Geometric Estimation Problems</a></div>
        <ul>
          <li><a href="#1dsurface">1D Surface Fitting</a></li>
          <li><a href="#2drobot">2D Robot Simulation</a></li>
        </ul>
        <div><a href="#discussion">Discussion</a></div>
      </nav>
    </d-contents> -->


    <p style="color:orange;">


      <b>Plan</b>
      <br>Begin with one demo from playground
      <br>Introduce spatial AI problem
      <br>Describe hardware problem
      <br>Introduce GBP and compare to other algorithms
      <br> Give intuitive explanations of GBP with interactive diagrams
      <br>Discuss future applications

      <br><br>
      <b>Notes:</b>

      1. Spatial AI

      2. Hardware:
      we have a strong feeling that algorithms which can operate with purely local computation and in-place data storage on a graph, and communicate via message passing, will fit well with coming computer hardware.


      3. Inference in Spatial AI and Factor graphs
        - Bayesian inference in Spatial AI
        - Graphical models
        - Factor graphs
        - Factor graphs as master rep
        - Gaussians
        - Gaussian posterior, sparsity relationship between information matrix and graph

      3. Factor graph as master representation:
          - Most accurate, non-linear, general
          - What we need are marginals
          - Enables dynamic computation
          - Batch solution always possible; practical algorithms for real-time either filter or choose
            subset of data.

      4. Probabilistic inference as manipulating a factor graph
        - How can current algorithms be seen as manipulations on a facotr graph-based
        - What manipulation aligns with our identification as of algorithm of message passing character from hardware section. 
        - GBP!
      
      4. GBP
        - Intuitiion
        - Equations


      Some thoughts from Andy 12/8/20
      - can we think of GBP as the most basic form of second order optimisation,
      of which other well known algorithms are special cases?
      - as parallelisation gets greater, and also as asynchronicity increases,
      does GBP get more and more advantageous?
      
      GBP with multiple hierarchies?


    </p>


    <hr>


    <h2 id="spatialAI">Spatial AI </h2>

    <p>
      We want to build robots that can understand and interact intelligently with their environments. 
      <i>Spatial AI</i> is the core capability of real-time vision-driven scene understanding that enables this type of behaviour within the constraints of practical physical systems.
      It is broadly the task of incremental estimation in which a persistent scene model is constructed and continually updated using data from a variety of sources.      
      These representations should capture the geometry, semantic properties, physical properties and other information about the environment. 
    </p>

    <p>
      We call this general spatial perception capability <i><b>Spatial AI</b></i> <d-cite key="davison2018futuremapping"></d-cite>. 
      We believe that SLAM will evolve into Spatial AI.
    </p>

    <p>
      What capabilities do we need: Consistency, long term persistence, abstraction and efficiency
    </p>
    <p>
      Why? To enable intelligence! 
      A robot will have a model of the world which is a simulation of reality based on all of its prior knowledge and ongoing experience; and be able to use it to make intelligent e.g. causal inferences about action. 
      I believe in persistent scene representation for spatial reasoning. The foundation for higher level AI?
    </p>

    <h3 id="slam">Current Landscape of SLAM Algorithms </h2>

    <p>
      We begin the discussion of how to build a system capable of Spatial AI by examining existing works in that direction. 
      Core capabilities are dense geometry and object class understanding: ElasticFusion, SemanticFusion: how to do it properly?
    </p>

    <p>
      Current prototype Spatial AI systems, attempting to process this heterogeneous flow of data into complicated persistent representations via various estimation techniques, often have severe performance bottlenecks due to limits in the capacities of computation load, data storage or data transfer. 
      Two lines of attack: representation and hardware.
      SemanticFusion is very complicated and requires heterogeneous computation and a lot of data movement.
    </p>

    <p>
      Paragraph laying out structure of the remainder of the article.
    </p>

    <h2 id="hardware"> Hardware</h2>

    <p>
      Data transfer is by far the most power hungry operation.
    </p>

    <figure class="l-body side">
      <img src="./diagrams/energy.svg"></img>
      <figcaption style="bottom: 0px;">Energy</figcaption>
    </figure>

    <p>
      Identify properties of performant system: high parallelism, but also distributed storage to keep data close to compute and reduce bits x mm.
    </p>

    <p>
      What hardware is used in current practical SLAM systems? 
      GPUs not well suited for computation in which we are constantly checking new measurements with stored model.
      They are generally used to speed up specific module. 
      Systems rely on large amounts of high bandwidth data transfer. 
      Cloud compute also used somewhat in industry as well as custom hardware.
    </p>

    <figure class="l-body">
      <d-figure id=""></d-figure>
      <figcaption>Toy visualisation of data flow and compute on current CPUs / GPUs.</figcaption>
    </figure>

    <p>
      Now hardware is being designed specifically for AI and even Spatial AI.
      Graph processors, such as the Graphcore's IPU make general design choices towards enabling the kind of local graph based processing required for many efficient AI applications.
      Algorithms which involve local compute and minimise data transfer will be efficient on this new wave of processing hardware.
      These algorithms would have a local message passing character on a graph. 
      The figure below illustrates the reduced data flow and increased parallelism that an algorithm based around local message passing can acheive on a graph processor.
    </p>

    <figure class="l-body">
      <d-figure id=""></d-figure>
      <figcaption>Toy visualisation of data flow and compute on graph processor.</figcaption>
    </figure>

    <h2>Factor graphs and Spatial AI</h2>

    <p>
      As we have described, our proposition is that algorithms which can operate with purely local computation and in-place data storage on a graph, 
      and communicate via message passing, will fit well with coming computer hardware.
      With this in mind, we now explore the graphs involved in Spatial AI problems through the link between probability distributions and graphical models.
    </p>  

    <h3>Graphical Models</h3>

    <p>
      Graphical models are a graph-based representation of a probability distribution which captures compactly the structure or factorisation of the distribution.
      They are useful as graphs provide a simple way to visualise the compositional structure of problems and additionally graphical manipulations provide a basis for understanding complex computations in these problems.
      Graphical models are comprised of nodes that represent a random variable (or subset of random variables) and edges that represent probabilistic relationships between the variables.
      <!-- There are directed (Bayesian networks), undirected (Markov Random Fields) and factor graphs.  -->
      They are particularly insightful when representing distributions over random variables with a sparse conditional dependence structure.
    </p>


    <d-math block="">
      P(X) = \prod_i P(x_i | pa_i)  \;\;\;\; \text{where} \;\;\;\; \forall \;\;pa_i , \;\;  |pa_i| \ll |X|
    </d-math>

    <div class=margin>
      <p>
        <d-math>pa_i</d-math> is the set of random variables on which <d-math>x_i</d-math> depends, or the parents.
      </p>
      <p>
        <d-math> |X|</d-math> denotes the cardinality of (number of elements in) the set <d-math>X</d-math>.
      </p>
    </div>


    <p>
      This sparse structure is inherent in spatial estimation problems due the property of <b>locality</b> - variables which are local (in space or time) tend to be dependent variables, while variables more separated spatially and temporally are likely to be independent.
      Hence, graphical models are a useful tool for inspecting the structure of Spatial AI problems and designing computationally efficient inference algorithms.
      <!-- Consider for example in bundle adjustment, the generative model for particular feature (measurement) is a function of only the pose of a single keyframe and the 3D location of the landmark. -->
    </p>

    <p>
      We now briefly discuss spatial estimation problems using Bayesian probability theory to understand the probability distributions we are interested in modelling graphically.
    </p>

    <h3>Bayesian Inference and Gaussian distributions</h3>

    <p>
      Before presenting the graphical representation of spatial estimation problems, we first discuss the probability distributions involved using Bayesian probability theory.
      Spatial estimation problems invovle the probabilistic fusion of multiple sources of uncertain data.
      More concretely, we are interested in characterising our knowledge about a set of random variables <d-math>X</d-math> that describe a representation of the world and/or an agent, given a set of noisy sensor measurements <d-math>Z</d-math>
      <d-footnote>To give an example, in the problem of pose graph optimisation, <d-math>X</d-math> are the poses and <d-math>Z</d-math> are the relative pose measurements. </d-footnote>.
      In the language of Bayesian probability theory, this amounts to performing probabilistic inference to determine the conditional posterior density <d-math>P(X | Z)</d-math>.
      In order to guide the inference component to learn a robust model <d-math>P(X | Z)</d-math>, we require a generative model or likelihood <d-math>P(Z | X)</d-math> which simulates sensor measurements given a configuration of <d-math>X</d-math>.
      The generation of sensor measurements is modelled by separating the generative model into a measurement function <d-math>h(X)</d-math> and a noise model <d-math>\eta</d-math>. 
    </p>

    <d-math block="">
      Z = h(X) + \eta
    </d-math>

    <p>
      For most spatial estimation problems, the physical process that generated the sensor measurements is well understood. We can therefore hand-design <d-math>h(X)</d-math> to simulate the measurement generation.
      Meanwhile, the noise model captures the random noise expected in our sensor readings.
      The generative model (likelihood) is related to the posterior which we want to estimate by Bayes' Rule:
    </p>

    <d-math block="">
      P(X|Z) = \frac{P(Z|X)P(X)}{P(Z)} 
    </d-math>

    <p>
      where <d-math>P(X)</d-math> is the prior on <d-math>X</d-math> and <d-math>P(Z)</d-math> is the marginal likelihood.
    </p>

    <p>
      The probability densities above can take any form, however it is commonly assumed in spatial estimation problems that all distributions are multivariate Gaussians
      <d-footnote>Measurement distributions often have longer tails, posterior distributions may be mulitmodal. </d-footnote>.
      Multivariate Gaussian distributions have two equivalent parametrisations: 
      the covariance form <d-math>\mathcal{N}(\mu, \Sigma)</d-math> and the information form <d-math>\mathcal{N}^{-1}(\eta, \Lambda)</d-math>. 
    </p>

    <d-math block="">
      \mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2 \pi)^k |\Sigma|}} \exp{(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu))}
    </d-math>

    <d-math block="">
      \mathcal{N}^{-1}(x; \eta, \Lambda) = \frac{1}{\sqrt{(2 \pi)^k |\Lambda^{-1}|}} \exp{(-\frac{1}{2} \eta^\top \Lambda^{-1} \eta)} \exp{(-\frac{1}{2} x^\top \Lambda x + \eta^\top x)}
    </d-math>

    <p>
      where <d-math>\Lambda = \Sigma^{-1}</d-math> and <d-math>\eta = \Sigma^{-1} \mu</d-math>.
    </p>
    <p>
      The assumption of a Gaussian measurement distribution is implemented by choosing a Gaussian noise model for the generative model <d-math>\eta \sim \mathcal{N}(0, \Sigma_n)</d-math>.
      This leads to a likelihood:
    </p>

    <d-math block="">
      P(Z|X) = \mathcal{N}(Z; h(X), \Sigma_n) = \frac{1}{\sqrt{(2 \pi)^k |\Sigma_n|}} \exp{(-\frac{1}{2} (Z - h(X))^\top \Sigma_n^{-1} (Z - h(X)))}
    </d-math>

    <p>
      Similarly using prior knowledge we can construct Gaussian distributions for the prior and marginal likelihood. 
      Following Bayes' Rule we can then recover the desired posterior.
    </p>

    <p>
      Now that we have established that we are modelling the posterior density <d-math>P(X|Z)</d-math>, we will discuss factor graphs as the graphical model of choice for representing this distribution.
    </p>

    <h3>Factor Graphs</h3>

    <p>
      Factor graphs are a bipartitie graphical model that have been commonly used to represent the posterior density in Spatial AI problems <d-cite key="Dellaert:Kaess:Foundations2017"></d-cite>.  
      They consist of a set of variable nodes <d-math>X = \{ x_i\}</d-math> for the numerical parameters of the system we want to model and a set of factor nodes <d-math>F = \{ f_i \}</d-math> each of which connect to a subset of the variable nodes <d-math>X_i\subset X</d-math>.
      Each factor describes a probabilistic constraint between a subset of the variables <d-math>X_i</d-math> with an arbitrary function <d-math>f_i(X_i)</d-math>.
      In this way a factor graph visually represents the factorisation of a global function <d-math>g(X)</d-math> as:
    </p>

    <d-math block="">
      g(X) = \prod_i f_i(X_i)
    </d-math>

    <p>
      By identifying the factorisation of the posterior density function we can use factor graphs to represent spatial estimation problems.
    </p>
    
    <figure class="eq-grid">

      <div style="grid-row: 1; grid-column: 1;">
        <d-math> P(X|Z) ~~~=~~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 2;">
        <d-math> \frac{P(Z|X) P(X)}{P(Z)}  </d-math>
      </div>

      <div style="grid-row: 2; grid-column: 1;">
        <d-math> ~~~~~~~~~~~~~~~~~~=~~~~ </d-math>
      </div>
      <div style="grid-row: 2; grid-column: 2;">
        <d-math> \frac{1}{P(Z)} \prod_i P(z_i | X_i) \prod_j P(x_j) </d-math>
      </div>

      <div style="grid-row: 3; grid-column: 1;">
        <d-math> ~~~~~~~~~~~~~~~~~~\propto~~~~ </d-math>
      </div>
      <div style="grid-row: 3; grid-column: 2;">
        <d-math>  \prod_i \psi(X_i;z_i) \prod_j P(x_j) </d-math>
      </div>

      <div style="grid-row: 4; grid-column: 1;">
        <d-math> ~~~~~~~~~~~~~~~~~~\propto~~~~ </d-math>
      </div>
      <div style="grid-row: 4; grid-column: 2;">
        <d-math>  \prod_i f(X_i)  </d-math>
      </div>

    </figure>

    <p>
      In going from the first to the second line, we assume that all measurements are independent and that the prior distribution of each variable is independent.
      As the measurements <d-math>Z</d-math> are given, we then drop <d-math>P(Z)</d-math>. 
      The likelihood <d-math>P(Z|X)</d-math> is a function of Z given a choice of X, however with <d-math>Z</d-math> given, we are interested in all densities as a function of X.
      Hence we define <d-math>\psi(X_i; z_i)</d-math> which is the likelihood as a function of X for the given measurements <d-math>Z</d-math>
      <d-footnote>
        Note that <d-math>\psi(X_i; z_i)</d-math> is not a normalised distribution. 
        In addition if <d-math>P(z_i | X_i)</d-math> is a Gaussian then <d-math>\psi(X_i; z_i)</d-math> is only Gaussian if the measurement function <d-math>h(X_i)</d-math> is linear.
      </d-footnote>.
      The final line illustrates that a function proportional to the posterior can be factorised into two types of factors: measurement factors and prior factors
      <d-footnote>The prior factors are often uniform distributions in which case they are independent of the variable and so can be dropped into the proportionality constant.</d-footnote>.
      Here we display an example of a factor graphs for ...
    </p>

    <figure class="l-page side">
      <img src="./diagrams/surfacegraph.svg"></img>
      <figcaption style="bottom: 0px;">Factor graph for...</figcaption>
    </figure>


    <h2>Factor Graphs and Probabilistic Inference</h2>
    
    <p>
      Generally, it can be computationally intractable or impractical to determinine the full normalised posterior density <d-math>P(X|Z)</d-math>.
      Instead, we are usually interested in answering queries about the posterior such as finding the mode or the marginal distributions for each variable.
      We will now discuss various inference techniques and use factor graphs to relate the techniques to our goal of finding an inference algorithm with a message passing character.
    </p>

    <h3>Manipulations on factor graphs</h3>

<!-- 
    <p>
      We choose to represent the posterior Gaussian in the information form as it exposes the one-to-one relationship between graphical models and matrices.
      A zero element in the covariance matrix means the two variables are marginally independent without observing the other variables.
      While a zero element in the information matrix means that the variables are conditionally independent given the other variables are observed.
      As a result, a non-zero element in the information matrix coresponds to an edge in a Markov Random Field and a factor that is adjacent to both variables in a factor graph.
      A MRF and factor graph is a diagrammatic repesentation of the sparsity structure in the information matrix.

    </p>

    <p class="margin">
      <d-math block="">
        \Sigma_{i,j} = 0 \;\; \Longleftrightarrow \;\;
      </d-math>

      <d-math block="">
        \Lambda_{i,j} = 0 \;\; \Longleftrightarrow \;\; P(x_i, x_j | X_{-ij}) = P(x_i | X_{-ij}) P(x_j | X_{-ij}) \;\; \Longleftrightarrow \;\; x_i \bot x_j | X_{-ij}
      </d-math>

    </p>

    <figure class="l-body">
        <figcaption>Illustration of information form and MRF? And factor graph?</figcaption>
    </figure> -->



    <p>
      Solving to find the mode of a Gaussian posterior is equivalent to least squares minimisation which involves solving a linear system (iteratively for a non-linear measurement function).
      The linear system is solved by inverting the information matrix.
      The sparsity structure of the information matrix is related to the factorisation of the posterior.
      This solution method can be understood graphically as collecting all of the factors globally before performing a global inversion.
    </p>

    <p>
      Many years of research has investigated how to exploit the sparsity in the information matrix to speed up the linear solver.
      All practical systems make assumptions about the structure of the factor graph.
      Some examples...
      Variable elimination
    </p>

    <p>
      iSAM is moving in the right direction and we share the vision of the factor graph as the master representation. 
      We believe that as we move towards solving more diverse spatial AI problems with more heterogeneous factors, we need to move away from making any assumptions.
      This leads us towards algorithms with local compute which also fit in with the characteristics we identified for new graph processing hardware. 
    </p>

    <p>
      Gaussian Belief propagation has exactly this character.
      It is a message passing algorithm which computes the marginals with only local compute on a factor graph. 
      We now enumerate the key arguments for the use of GBP in Spatail AI.
    </p>

    <h2 id="gbp">Why Gaussian Belief Propagation? </h2>

    <p>
      We argue that Gaussian Belief Propagation (GBP) is a strong algorithmic framework for the distributed, generic and incremental probabilistic estimation that we need in Spatial AI. 
      Here we describe how GBP can be the foundation for high performance smart robots and devices which operate within the constraints of real products.
    </p>


    <h3>Factor graphs as the master representation</h3>

    <p>
      We advocate the factor graphs not only a useful tool for inspecting problems and computation but that factor graphs are a promising choice for the master representation of the posterior density in Spatial AI problems for the following reasons.
      <ul>
        <li>
          They are completely general and can represent any non-linear distribution or function.
        </li>
        <li>
          They enable and encourage dynamic, parallel, in-place computation. 
          By thinking about computation as manipulations on a factor graph.
          Practical algorithms for real-time either filter or choose subset of data.
        </li>
        <li>
          Despite the above, doing centralised computation is always possible (e.g. taking the batch solution).
        </li>
        <li>
          What we need are marginals which is local property.
        </li>
      </ul>
    
    </p>

    <h3>Low power requires distributed computation and storage to fit new hardware
    </h3>
    <p> Sending data is costly in power comsumption. </p>
    <p>-> Message passing between processors with local computation and memory, either on a single chip or across multiple simple devices.</p> 

    <h3> Probabilistic fusion of heterogeneous inputs</h3>
    <p> A robot will need to probabilistically fuse different measurements and priors to jointly estimate its current state and optimise its representation of the environment. Bayesian probability theory, from which Gaussian Belief Propagation can be derived, provides a framework to combine constraints and perform probabilistic inference. </p> 

    <h3>We can build persistent and flexible representations from incremental measurements</h3>
    <p> <b>Flexibility</b>: Factor graphs can hold any combination of arbitary entities. This enables a natural interaction between different levels of hierarchy in a representation, such as sparse geometry and dense semantic information. </p>
    <p> <b>Persistence</b>: Full joint optimisation is ongoing in the background. Attention can bring into focus area of map. How is this an advantage over LM? </p>
    <p>-> Changing, irregular structure of measurements represented by a factor graph</p> 
    <p>-> Belief propagation: probabilistic message passing for in-place, convergent inference on a factor graph</p> 

    <h3>Gaussian: most efficient way to represent probability distributions</h3>
    <p>
      -> Gaussian Belief Propagation: messages take the form of small information vectors and matrices; all the same local operations as in Kalman Filtering or non-linear optimisation.
    </p>

    
    <h2 id="theory">Brief introduction to GBP </h2>

    <h3> Intuitive introduction </h3>

    <p>
      Spring model.
    </p>

    <figure class="l-page">
      <d-figure id="spring"></d-figure>
    </figure>


    <!-- <p>
      Visualisation of messages.
    </p>

    <figure class="l-page">
      <d-figure id="gaussprod"></d-figure>
        <figcaption>
          A landmark is observed by two different cameras, each of which makes an uncertain measurement of the distance to the landmark and the bearing. 
          The location of the cameras are well-known with high confidence. You can change the 
        </figcaption>
    </figure> -->


    <h3> Technical introduction </h3>

    <p>Belief Propagation is a commonly used inference algorithm
      <d-footnote>It is an exact inference algorithm for tree graphs.</d-footnote>
      for estimating the marginal distribution for a set of variables from the joint probability distribution.
      <d-footnote>Computing marginal distribution is equivalent to MAP with Gaussians. The MAP of a distribution is the mode, which for a Gaussian is simply the mean. So the MAP is the mean of the marginal distribution. </d-footnote> : </p>

    <d-math block="">
      p(\bold{x}_i) = \int p(\bold{x}) d\bold{x}_1 ... d\bold{x}_{i-1} d\bold{x}_{i+1} ... d\bold{x}_{n}
    </d-math>

    <p>
      It operates by iteratively sending messages between adjacent variables in the factor graph. 
      Messages are computed using the following equations 
      <d-footnote>See <d-cite key="Bishop:Book2006"></d-cite> or <d-cite key="Davison:Ortiz:ARXIV2019"></d-cite> for a detailled derivation.</d-footnote>. 
    </p>


    <p>Belief propagation can also be understood as finding from the variational inference perspective as finding a distribution <d-math>q(x)</d-math> which minimises the <d-math>KL(q || p)</d-math> subject to two constraints.
      <d-footnote>The two constraints are that <d-math>q(x)</d-math> is correctly normalised (<d-math>\sum_i q(x_i) = 1</d-math>) and is locally consistent (<d-math>q(x_i) = \sum_{x}</d-math>). </d-footnote>. </p>

    <figure class="eq-grid">

      <div style="grid-row: 1; grid-column: 1;">
        <d-math> ~~~~~\mu_{f_j \rightarrow x_i} </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 2;">
        <d-math>  ~=~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 3;">
        <d-math>  ~~~~~~~~\sum_{X_j \setminus x_i} </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 4;">
        <d-math>  ~~~~f(X_j)~~~~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 5;">
        <d-math> ~~~~\prod_{k, x_k \in n(f_j) \setminus x_i}~~~ </d-math>
      </div>

      <div style="grid-row: 1; grid-column: 6;">
        <d-math> ~~~\mu_{x_k \rightarrow f_j} </d-math>
      </div>

      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 1 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 3 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 4 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 5 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 6 / span 1; "></div>
  
      <figcaption style="grid-row: 3; grid-column: 1; max-width:110px;">
        Factor to variable message
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 3; max-width:145px;">
        Marginalise over all other adjacent variables
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 4; max-width:100px;">
        Factor potential
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 5; max-width:130px;">
        Product over all other adjacent variables
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 6; max-width:85px;">
        Message from variable <d-math>x_k</d-math><br>
      </figcaption>


      <div style="grid-row: 4; grid-column: 1;">
        <d-math> ~~~~~\mu_{x_i \rightarrow f_j} </d-math>
      </div>
      <div style="grid-row: 4; grid-column: 2;">
        <d-math>  ~=~~~ </d-math>
      </div>
      <div style="grid-row: 4; grid-column: 3;">
        <d-math> ~~~~\prod_{s, f_s \in n(x_i) \setminus f_j}~~~ </d-math>
      </div>

      <div style="grid-row: 4; grid-column: 4;">
        <d-math> ~~~\mu_{f_s \rightarrow x_i} </d-math>
      </div>

      <div class="expansion-marker-below" style="grid-row: 5; grid-column: 1 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 5; grid-column: 3 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 5; grid-column: 4 / span 1; "></div>
  
      <figcaption style="grid-row: 6; grid-column: 1; max-width:110px;">
        Variable to factor message
      </figcaption>
      <figcaption style="grid-row: 6; grid-column: 3; max-width:140px;">
        Product over all other adjacent factors
      </figcaption>
      <figcaption style="grid-row: 6; grid-column: 4; max-width:85px;">
        Message from factor <d-math>f_s</d-math><br>
      </figcaption>

    </figure>

    <figure class="margin">
      <p> <d-math>X_j=n(f_j)</d-math> is the set of variables adjacent to factor <d-math>f_j</d-math></p>
      <p> The summation becomes an integral for continuous variables </p>
    </figure>

    <p>
      On convergence the marginal distribution is evaluated as follows:
    </p>

    <figure class="eq-grid">

      <div style="grid-row: 1; grid-column: 1;">
        <d-math> ~~~~p(x_i)~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 2;">
        <d-math> ~~~=~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 3;">
        <d-math> ~~~\prod_{s \in n(x_i)}~~~~~ </d-math>
      </div>
      <div style="grid-row: 1; grid-column: 4;">
        <d-math> ~~\mu_{f_s \rightarrow x_i} </d-math>
      </div>

      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 1 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 3 / span 1; "></div>
      <div class="expansion-marker-below" style="grid-row: 2; grid-column: 4 / span 1; "></div>
  

      <figcaption style="grid-row: 3; grid-column: 1; max-width:85px;">
        Marginal distribution
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 3; max-width:100px;">
        Product over adjacent factors
      </figcaption>
      <figcaption style="grid-row: 3; grid-column: 4; max-width:85px;">
        Message from factor <d-math>f_s</d-math><br>
      </figcaption>

    </figure>




    <p>
      Works the same with Gaussians - all variables are Gaussian distributions, as are messages and factors.
    </p>

    <p>
      Loopy belief propagation is an approximate inference algorithm. 
    </p>

    <h3>Other comments</h3>

    <p>
      <ul>
        <li>
          <b>Non-linear measurement functions.</b> 
          Relinearisation. 
          Can we visualise the affect that relinearisation has on a message. 
          i.e. it turns a non Gaussian factor distribution into a Gaussian by moment matching.
        </li>

        <li>
          <b>Message passing schedule.</b> 
          On trees, convergence is acheived in one sweep from root to leaves and back up. 
          On loopy graphs there are various choices for scheduling messages.
          Synchronous message passing along all edges is a popular choice, while other heuristics based on information theory may also converge quickly while reducing computation.
        </li>

        <li>
          <b>Convergence.</b>
          Like other optimisation algorithms it may not always find the global minima.
          Several heuristics just as damping can help <d-cite key="koller2009PGMs"></d-cite>.
        </li>
      </ul>

    </p>


    <h2 id="simulations">Simulated estimation problems </h2>


    <h3 id="1dsurface">1D Surface estimation</h3>

    <p> How to formulate the measurement and smoothness factors. </p>

    <figure class="l-page-outset">
      <d-figure id="surface_fitting"></d-figure>
    </figure>

    <h3 id="1dsurface">GBP Playground</h3>

    These are essentially different types of pose graph estimation problems.

    <figure class="l-page-outset">
      <d-figure id="playground"></d-figure>
    </figure>
  


    <h2 id="discussion">Discussion</h2>
    
    <p> Bundle Adjustment <d-cite key="ortiz2020gbp"></d-cite></p>


    <h3> Incorporating learning</h3>
    <p> 
      We have a known causal generative model, this guides the encoder (GBP) to learn robust models of X. 
      Inference is inverse modelling, where we try to infer some quantities that explain the observations. It is an iterative encoder. 
      Probabilistic inference, akin to an encoder network, estimates <d-math>P(X|Z)</d-math>
      GBP is a causal / physical generative model that models the conditional probability <d-math>P(X|Z)</d-math>. 
      We can use these models for a robot trajectory or a heightmap fitting problem.
      However modeling the generative process for something like an image is more difficult, we must model hierarchies and self similarity.
      The graphical model for something like an image is going to be more densely connected, so using graphical models becomes less illustrative. 

      They identify that GBP may fail if we do not know the full data generation process or if we have loopy graphs.
      'Neural augmentation', embedding a generative causal model into the structure of a NN <d-cite key="kuckWelling2020NEBP"></d-cite>. 
      This is the idea of neural enhanced BP.
    </p>

  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> 
    </p>

    <p>
      <b>Writing & Diagrams:</b> 
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
